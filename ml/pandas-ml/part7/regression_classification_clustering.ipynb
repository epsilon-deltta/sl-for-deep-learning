{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "regression_classification_clustering.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/epsilon-deltta/pandas_data_analytics_source/blob/master/part7/regression_classification_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad35hJMtAtP1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import time\n",
        "import statsmodels.api as sm\n",
        "from sklearn import linear_model\n",
        "from tqdm import tnrange, tqdm_notebook\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from statsmodels.regression.quantile_regression import QuantReg\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0USKGbTZ3Vt"
      },
      "source": [
        "#### multivariate regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AImLlS-eZvLA"
      },
      "source": [
        "'''\r\n",
        "[Step 1 ~ 3] 데이터 준비 \r\n",
        "'''\r\n",
        "# CSV 파일을 데이터프레임으로 변환\r\n",
        "df = pd.read_csv('./auto-mpg.csv', header=None)\r\n",
        "\r\n",
        "# 열 이름 지정\r\n",
        "df.columns = ['mpg','cylinders','displacement','horsepower','weight',\r\n",
        "              'acceleration','model year','origin','name'] \r\n",
        "\r\n",
        "# horsepower 열의 자료형 변경 (문자열 ->숫자)\r\n",
        "df['horsepower'].replace('?', np.nan, inplace=True)      # '?'을 np.nan으로 변경\r\n",
        "df.dropna(subset=['horsepower'], axis=0, inplace=True)   # 누락데이터 행을 삭제\r\n",
        "df['horsepower'] = df['horsepower'].astype('float')      # 문자열을 실수형으로 변환\r\n",
        "\r\n",
        "# 분석에 활용할 열(속성)을 선택 (연비, 실린더, 출력, 중량)\r\n",
        "ndf = df[['mpg', 'cylinders', 'horsepower', 'weight']]\r\n",
        "\r\n",
        "\r\n",
        "'''\r\n",
        "Step 4: 데이터셋 구분 - 훈련용(train data)/ 검증용(test data)\r\n",
        "'''\r\n",
        "\r\n",
        "# 속성(변수) 선택\r\n",
        "X=ndf[['cylinders', 'horsepower', 'weight']]  #독립 변수 X1, X2, X3\r\n",
        "y=ndf['mpg']     #종속 변수 Y\r\n",
        "\r\n",
        "sns.pairplot(data=ndf)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwnrcjO1Z1Px"
      },
      "source": [
        "\r\n",
        "# train data 와 test data로 구분(7:3 비율)\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10) \r\n",
        "\r\n",
        "print('훈련 데이터: ', X_train.shape)\r\n",
        "print('검증 데이터: ', X_test.shape)   \r\n",
        "print('\\n') \r\n",
        "\r\n",
        "\r\n",
        "'''\r\n",
        "Step 5: 다중회귀분석 모형 - sklearn 사용\r\n",
        "'''\r\n",
        "\r\n",
        "# sklearn 라이브러리에서 선형회귀분석 모듈 가져오기\r\n",
        "from sklearn.linear_model import LinearRegression\r\n",
        "\r\n",
        "# 단순회귀분석 모형 객체 생성\r\n",
        "lr = LinearRegression()   \r\n",
        "\r\n",
        "# train data를 가지고 모형 학습\r\n",
        "lr.fit(X_train, y_train)\r\n",
        "\r\n",
        "# 학습을 마친 모형에 test data를 적용하여 결정계수(R-제곱) 계산\r\n",
        "r_square = lr.score(X_test, y_test)\r\n",
        "print(r_square)\r\n",
        "print('\\n')\r\n",
        "\r\n",
        "# 회귀식의 기울기\r\n",
        "print('X 변수의 계수 a: ', lr.coef_)\r\n",
        "print('\\n')\r\n",
        "\r\n",
        "# 회귀식의 y절편\r\n",
        "print('상수항 b', lr.intercept_)\r\n",
        "print('\\n')\r\n",
        "\r\n",
        "# train data의 산점도와 test data로 예측한 회귀선을 그래프로 출력 \r\n",
        "y_hat = lr.predict(X_test)\r\n",
        "\r\n",
        "# yhat and y_test\r\n",
        "plt.figure(figsize=(10, 5))\r\n",
        "ax1 = sns.distplot(y_test, hist=False, label=\"y_test\")\r\n",
        "ax2 = sns.distplot(y_hat, hist=False, label=\"y_hat\", ax=ax1)\r\n",
        "plt.show()\r\n",
        "plt.close()\r\n",
        "\r\n",
        "# normality test shapiro,k -s teste \r\n",
        "from scipy.stats import shapiro , normaltest , anderson , kstest\r\n",
        "\r\n",
        "residual = y_hat-y_test\r\n",
        "\r\n",
        "sha_stat, sha_p  = shapiro(residual)\r\n",
        "k_stat, k_p  = kstest(residual,\"norm\")\r\n",
        "print(f'{sha_p}  :  {k_p}')\r\n",
        "\r\n",
        "### q-qplot\r\n",
        "plt.scatter(y_test,y_hat)\r\n",
        "plt.plot([0,40],[0,40])\r\n",
        "\r\n",
        "plt.boxplot(residual)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7hsbYV-AtP6"
      },
      "source": [
        "#### classification DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_39f2SmkAtP6"
      },
      "source": [
        "'''\n",
        "[Step 1] 데이터 준비/ 기본 설정\n",
        "'''\n",
        "\n",
        "# load_dataset 함수를 사용하여 데이터프레임으로 변환\n",
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "#  IPython 디스플레이 설정 - 출력할 열의 개수 한도 늘리기\n",
        "pd.set_option('display.max_columns', 15)\n",
        "\n",
        "\n",
        "'''\n",
        "[Step 2] 데이터 탐색/ 전처리\n",
        "'''\n",
        "\n",
        "# NaN값이 많은 deck 열을 삭제, embarked와 내용이 겹치는 embark_town 열을 삭제\n",
        "rdf = df.drop(['deck', 'embark_town'], axis=1)  \n",
        "\n",
        "# age 열에 나이 데이터가 없는 모든 행을 삭제 - age 열(891개 중 177개의 NaN 값)\n",
        "rdf = rdf.dropna(subset=['age'], how='any', axis=0)  \n",
        "\n",
        "# embarked 열의 NaN값을 승선도시 중에서 가장 많이 출현한 값으로 치환하기\n",
        "most_freq = rdf['embarked'].value_counts(dropna=True).idxmax()   \n",
        "rdf['embarked'].fillna(most_freq, inplace=True)\n",
        "\n",
        "\n",
        "'''\n",
        "[Step 3] 분석에 사용할 속성을 선택\n",
        "'''\n",
        "\n",
        "# 분석에 활용할 열(속성)을 선택 \n",
        "ndf = rdf[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'embarked']]\n",
        "\n",
        "# 원핫인코딩 - 범주형 데이터를 모형이 인식할 수 있도록 숫자형으로 변환\n",
        "onehot_sex = pd.get_dummies(ndf['sex'])\n",
        "ndf = pd.concat([ndf, onehot_sex], axis=1)\n",
        "\n",
        "onehot_embarked = pd.get_dummies(ndf['embarked'], prefix='town')\n",
        "ndf = pd.concat([ndf, onehot_embarked], axis=1)\n",
        "\n",
        "ndf.drop(['sex', 'embarked'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "'''\n",
        "[Step 4] 데이터셋 구분 - 훈련용(train data)/ 검증용(test data)\n",
        "'''\n",
        "\n",
        "# 속성(변수) 선택\n",
        "X=ndf[['pclass', 'age', 'sibsp', 'parch', 'female', 'male', \n",
        "       'town_C', 'town_Q', 'town_S']]  #독립 변수 X\n",
        "y=ndf['survived']                      #종속 변수 Y\n",
        "\n",
        "# 설명 변수 데이터를 정규화(normalization)\n",
        "from sklearn import preprocessing\n",
        "x = preprocessing.StandardScaler().fit(X).transform(X)\n",
        "\n",
        "# train data 와 test data로 구분(7:3 비율)\n",
        "from sklearn.model_selection import train_test_split\n",
        "xtr, xte, ytr, yte = train_test_split(x, y, test_size=0.3, random_state=10) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC_EMhnEAtP7"
      },
      "source": [
        "#### SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCoLav0BAtP7"
      },
      "source": [
        "'''\n",
        "[Step 5] SVM 분류 모형 - sklearn 사용\n",
        "'''\n",
        "\n",
        "# sklearn 라이브러리에서 SVM 분류 모형 가져오기\n",
        "from sklearn import svm\n",
        "\n",
        "# 모형 객체 생성 (kernel='rbf' 적용)\n",
        "svm_model = svm.SVC(kernel='rbf')\n",
        "\n",
        "# train data를 가지고 모형 학습\n",
        "svm_model.fit(xtr, ytr)   \n",
        "\n",
        "# test data를 가지고 y_hat을 예측 (분류) \n",
        "yhat = svm_model.predict(xte)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x9XDaRyAtP7",
        "outputId": "47ea88cb-3d21-41d5-c01d-8bb797f78c23"
      },
      "source": [
        "# 모형 성능 평가 - Confusion Matrix 계산\n",
        "from sklearn import metrics \n",
        "svm_matrix = metrics.confusion_matrix(yte, yhat)  \n",
        "print(svm_matrix)\n",
        "print('\\n')\n",
        "\n",
        "# 모형 성능 평가 - 평가지표 계산\n",
        "svm_report = metrics.classification_report(yte, yhat)            \n",
        "print(svm_report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[120   5]\n",
            " [ 35  55]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.96      0.86       125\n",
            "           1       0.92      0.61      0.73        90\n",
            "\n",
            "    accuracy                           0.81       215\n",
            "   macro avg       0.85      0.79      0.80       215\n",
            "weighted avg       0.83      0.81      0.81       215\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CM96OlTAtP-"
      },
      "source": [
        "#### KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHpVg3p1AtP_",
        "outputId": "3248b6c5-f622-44ee-9745-762b3b9f7fd4"
      },
      "source": [
        "\n",
        "# sklearn 라이브러리에서 KNN 분류 모형 가져오기\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# 모형 객체 생성 (k=5로 설정)\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# train data를 가지고 모형 학습\n",
        "knn.fit(xtr, ytr)   \n",
        "\n",
        "# test data를 가지고 y_hat을 예측 (분류) \n",
        "yhat = knn.predict(xte)\n",
        "\n",
        "\n",
        "# 모형 성능 평가 - Confusion Matrix 계산\n",
        "from sklearn import metrics \n",
        "knn_matrix = metrics.confusion_matrix(yte, yhat)  \n",
        "print(knn_matrix)\n",
        "\n",
        "# 모형 성능 평가 - 평가지표 계산\n",
        "knn_report = metrics.classification_report(yte, yhat)            \n",
        "print(knn_report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[109  16]\n",
            " [ 25  65]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       125\n",
            "           1       0.80      0.72      0.76        90\n",
            "\n",
            "    accuracy                           0.81       215\n",
            "   macro avg       0.81      0.80      0.80       215\n",
            "weighted avg       0.81      0.81      0.81       215\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux4VswssAtQA"
      },
      "source": [
        "#### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYHCucJ-AtQB",
        "outputId": "45b57437-e8f2-464e-de7f-623ea16b6ea6"
      },
      "source": [
        "# sklearn 라이브러리에서 Decision Tree 분류 모형 가져오기\n",
        "from sklearn import tree\n",
        "\n",
        "# 모형 객체 생성 (criterion='entropy' 적용)\n",
        "tree_model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=5)\n",
        "\n",
        "# train data를 가지고 모형 학습\n",
        "tree_model.fit(xtr, ytr)   \n",
        "\n",
        "# test data를 가지고 y_hat을 예측 (분류) \n",
        "yhat = tree_model.predict(xte)      # 2: benign(양성), 4: malignant(악성)\n",
        "\n",
        "\n",
        "# 모형 성능 평가 - Confusion Matrix 계산\n",
        "from sklearn import metrics \n",
        "tree_matrix = metrics.confusion_matrix(yte, yhat)  \n",
        "print(tree_matrix)\n",
        "print('\\n')\n",
        "\n",
        "# 모형 성능 평가 - 평가지표 계산\n",
        "tree_report = metrics.classification_report(yte, yhat)            \n",
        "print(tree_report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[120   5]\n",
            " [ 35  55]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.96      0.86       125\n",
            "           1       0.92      0.61      0.73        90\n",
            "\n",
            "    accuracy                           0.81       215\n",
            "   macro avg       0.85      0.79      0.80       215\n",
            "weighted avg       0.83      0.81      0.81       215\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXqiXZqbAtQB"
      },
      "source": [
        "## Clustering "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivU9M7yKAtQC"
      },
      "source": [
        "#### kmeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6GLcbWoAtQC"
      },
      "source": [
        "'''\n",
        "[Step 1] 데이터 준비\n",
        "'''\n",
        "\n",
        "# Wholesale customers 데이터셋 가져오기 (출처: UCI ML Repository)\n",
        "uci_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/\\\n",
        "00292/Wholesale%20customers%20data.csv'\n",
        "df = pd.read_csv(uci_path, header=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5hnqknyAtQC"
      },
      "source": [
        "'''\n",
        "[Step 3] 데이터 전처리\n",
        "'''\n",
        "\n",
        "# 분석에 사용할 속성을 선택\n",
        "X = df.iloc[:, :]\n",
        "\n",
        "# 설명 변수 데이터를 정규화\n",
        "from sklearn import preprocessing\n",
        "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "[Step 4] k-means 군집 모형 - sklearn 사용\n",
        "'''\n",
        "\n",
        "# sklearn 라이브러리에서 cluster 군집 모형 가져오기\n",
        "from sklearn import cluster\n",
        "\n",
        "# 모형 객체 생성 \n",
        "kmeans = cluster.KMeans(init='k-means++', n_clusters=5, n_init=10)\n",
        "\n",
        "# 모형 학습\n",
        "kmeans.fit(X)   \n",
        "\n",
        "# 예측 (군집) \n",
        "cluster_label = kmeans.labels_   \n",
        "print(cluster_label)\n",
        "\n",
        "# 예측 결과를 데이터프레임에 추가\n",
        "df['Cluster'] = cluster_label\n",
        "\n",
        "# 그래프로 표현 - 시각화\n",
        "df.plot(kind='scatter', x='Grocery', y='Frozen', c='Cluster', cmap='Set1', \n",
        "        colorbar=False, figsize=(10, 10))\n",
        "df.plot(kind='scatter', x='Milk', y='Delicassen', c='Cluster', cmap='Set1', \n",
        "        colorbar=True, figsize=(10, 10))\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# 큰 값으로 구성된 클러스터(0, 4)를 제외 - 값이 몰려 있는 구간을 자세하게 분석\n",
        "mask = (df['Cluster'] == 0) | (df['Cluster'] == 4)\n",
        "ndf = df[~mask]\n",
        "\n",
        "ndf.plot(kind='scatter', x='Grocery', y='Frozen', c='Cluster', cmap='Set1', \n",
        "        colorbar=False, figsize=(10, 10))\n",
        "ndf.plot(kind='scatter', x='Milk', y='Delicassen', c='Cluster', cmap='Set1', \n",
        "        colorbar=True, figsize=(10, 10))\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXnb48M1AtQE"
      },
      "source": [
        "!wget -o 2016_middle_shcool_graduates_report.xlsx https://drive.google.com/file/d/1DWLUDDX6_-QHDMrtS2nx8Ykb2prT7R7r/view?usp=sharing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I1TeFlpAtQG",
        "outputId": "2265bd0f-0894-43f0-c915-f438c4c72f79"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "### 기본 라이브러리 불러오기\n",
        "import pandas as pd\n",
        "import folium\n",
        "\n",
        "\n",
        "'''\n",
        "[Step 1] 데이터 준비/ 기본 설정\n",
        "'''\n",
        "\n",
        "# 서울시내 중학교 진학률 데이터셋 (출처: 교육???)\n",
        "file_path = './2016_middle_shcool_graduates_report.xlsx'\n",
        "df = pd.read_excel(file_path, header=0,engine='openpyxl')\n",
        "\n",
        "# IPython Console 디스플레이 옵션 설정하기\n",
        "pd.set_option('display.width', None)        # 출력화면의 너비\n",
        "pd.set_option('display.max_rows', 100)      # 출력할 행의 개수 한도\n",
        "pd.set_option('display.max_columns', 10)    # 출력할 열의 개수 한도\n",
        "pd.set_option('display.max_colwidth', 20)   # 출력할 열의 너비\n",
        "pd.set_option('display.unicode.east_asian_width', True)   # 유니코드 사용 너비 조정\n",
        "\n",
        "'''\n",
        "[Step 2] 데이터 탐색\n",
        "'''\n",
        "# 지도에 위치 표시\n",
        "mschool_map = folium.Map(location=[37.55,126.98], tiles='Stamen Terrain', \n",
        "                        zoom_start=12)\n",
        "\n",
        "# 중학교 위치정보를 CircleMarker로 표시\n",
        "for name, lat, lng in zip(df.학교명, df.위도, df.경도):\n",
        "    folium.CircleMarker([lat, lng],\n",
        "                        radius=5,              # 원의 반지름\n",
        "                        color='brown',         # 원의 둘레 색상\n",
        "                        fill=True,\n",
        "                        fill_color='coral',    # 원을 채우는 색\n",
        "                        fill_opacity=0.7,      # 투명도    \n",
        "                        popup=name\n",
        "    ).add_to(mschool_map)\n",
        "\n",
        "# 지도를 html 파일로 저장하기\n",
        "mschool_map.save('./seoul_mschool_location.html')\n",
        "\n",
        "\n",
        "'''\n",
        "[Step 3] 데이터 전처리\n",
        "'''\n",
        "\n",
        "# 원핫인코딩(더미 변수)\n",
        "from sklearn import preprocessing    \n",
        "\n",
        "label_encoder = preprocessing.LabelEncoder()     # label encoder 생성\n",
        "onehot_encoder = preprocessing.OneHotEncoder()   # one hot encoder 생성\n",
        "\n",
        "onehot_location = label_encoder.fit_transform(df['지역'])\n",
        "onehot_code = label_encoder.fit_transform(df['코드'])\n",
        "onehot_type = label_encoder.fit_transform(df['유형'])\n",
        "onehot_day = label_encoder.fit_transform(df['주야'])\n",
        "\n",
        "df['location'] = onehot_location\n",
        "df['code'] = onehot_code\n",
        "df['type'] = onehot_type\n",
        "df['day'] = onehot_day\n",
        "\n",
        "print(df.head())   \n",
        "print('\\n')\n",
        "\n",
        "\n",
        "'''\n",
        "[Step 4] DBSCAN 군집 모형 - sklearn 사용\n",
        "'''\n",
        "\n",
        "# sklearn 라이브러리에서 cluster 군집 모형 가져오기 \n",
        "from sklearn import cluster\n",
        "\n",
        "# 분석에 사용할 속성을 선택 (과학고, 외고국제고, 자사고 진학률)\n",
        "columns_list = [9, 10, 13]\n",
        "X = df.iloc[:, columns_list]\n",
        "\n",
        "# 설명 변수 데이터를 정규화\n",
        "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
        "\n",
        "# DBSCAN 모형 객체 생성\n",
        "dbm = cluster.DBSCAN(eps=0.2, min_samples=5)\n",
        "\n",
        "# 모형 학습\n",
        "dbm.fit(X)   \n",
        " \n",
        "# 예측 (군집) \n",
        "cluster_label = dbm.labels_   \n",
        "\n",
        "# 예측 결과를 데이터프레임에 추가\n",
        "df['Cluster'] = cluster_label\n",
        "\n",
        "# 클러스터 값으로 그룹화하고, 그룹별로 내용 출력 (첫 5행만 출력)\n",
        "grouped_cols = [0, 1, 3] + columns_list\n",
        "grouped = df.groupby('Cluster')\n",
        "# for key, group in grouped:\n",
        "#     print('* key :', key)\n",
        "#     print('* number :', len(group))    \n",
        "#     print(group.iloc[:, grouped_cols].head())\n",
        "#     print('\\n')\n",
        "\n",
        "# 그래프로 표현 - 시각화\n",
        "colors = {-1:'gray', 0:'coral', 1:'blue', 2:'green', 3:'red', 4:'purple', \n",
        "          5:'orange', 6:'brown', 7:'brick', 8:'yellow', 9:'magenta', 10:'cyan',11:'pink'}\n",
        "\n",
        "cluster_map = folium.Map(location=[37.55,126.98], tiles='Stamen Terrain', \n",
        "                        zoom_start=12)\n",
        "\n",
        "for name, lat, lng, clus in zip(df.학교명, df.위도, df.경도, df.Cluster):  \n",
        "    folium.CircleMarker([lat, lng],\n",
        "                        radius=5,                   # 원의 반지름\n",
        "                        color=colors[clus],         # 원의 둘레 색상\n",
        "                        fill=True,\n",
        "                        fill_color=colors[clus],    # 원을 채우는 색\n",
        "                        fill_opacity=0.7,           # 투명도    \n",
        "                        popup=name\n",
        "    ).add_to(cluster_map)\n",
        "\n",
        "# 지도를 html 파일로 저장하기\n",
        "cluster_map.save('./seoul_mschool_cluster.html')\n",
        "\n",
        "\n",
        "# X2 데이터셋에 대하여 위의 과정을 반복(과학고, 외고국제고, 자사고 진학률 + 유형)\n",
        "columns_list2 = [9, 10, 13, 22]\n",
        "X2 = df.iloc[:, columns_list2]\n",
        "# print(X2[:5])\n",
        "# print('\\n')\n",
        "\n",
        "\n",
        "X2 = preprocessing.StandardScaler().fit(X2).transform(X2)\n",
        "dbm2 = cluster.DBSCAN(eps=0.2, min_samples=5)\n",
        "dbm2.fit(X2)  \n",
        "df['Cluster2'] = dbm2.labels_   \n",
        "\n",
        "grouped2_cols = [0, 1, 3] + columns_list2\n",
        "grouped2 = df.groupby('Cluster2')\n",
        "# for key, group in grouped2:\n",
        "#     print('* key :', key)\n",
        "#     print('* number :', len(group))    \n",
        "#     print(group.iloc[:, grouped2_cols].head())\n",
        "#     print('\\n')\n",
        "\n",
        "cluster2_map = folium.Map(location=[37.55,126.98], tiles='Stamen Terrain', \n",
        "                        zoom_start=12)\n",
        "\n",
        "for name, lat, lng, clus in zip(df.학교명, df.위도, df.경도, df.Cluster2):  \n",
        "    folium.CircleMarker([lat, lng],\n",
        "                        radius=5,                   # 원의 반지름\n",
        "                        color=colors[clus],         # 원의 둘레 색상\n",
        "                        fill=True,\n",
        "                        fill_color=colors[clus],    # 원을 채우는 색\n",
        "                        fill_opacity=0.7,           # 투명도    \n",
        "                        popup=name\n",
        "    ).add_to(cluster2_map)\n",
        "\n",
        "# 지도를 html 파일로 저장하기\n",
        "cluster2_map.save('./seoul_mschool_cluster2.html')\n",
        "\n",
        "\n",
        "# X3 데이터셋에 대하여 위의 과정을 반복(과학고, 외고_국제고)\n",
        "columns_list3 = [9, 10]\n",
        "X3 = df.iloc[:, columns_list3]\n",
        "# print(X3[:5])\n",
        "# print('\\n')\n",
        "\n",
        "X3 = preprocessing.StandardScaler().fit(X3).transform(X3)\n",
        "dbm3 = cluster.DBSCAN(eps=0.2, min_samples=5)\n",
        "dbm3.fit(X3)  \n",
        "df['Cluster3'] = dbm3.labels_   \n",
        "\n",
        "grouped3_cols = [0, 1, 3] + columns_list3\n",
        "grouped3 = df.groupby('Cluster3')\n",
        "# for key, group in grouped3:\n",
        "#     print('* key :', key)\n",
        "#     print('* number :', len(group))    \n",
        "#     print(group.iloc[:, grouped3_cols].head())\n",
        "#     print('\\n')\n",
        "\n",
        "cluster3_map = folium.Map(location=[37.55,126.98], tiles='Stamen Terrain', \n",
        "                        zoom_start=12)\n",
        "\n",
        "for name, lat, lng, clus in zip(df.학교명, df.위도, df.경도, df.Cluster3):  \n",
        "    folium.CircleMarker([lat, lng],\n",
        "                        radius=5,                   # 원의 반지름\n",
        "                        color=colors[clus],         # 원의 둘레 색상\n",
        "                        fill=True,\n",
        "                        fill_color=colors[clus],    # 원을 채우는 색\n",
        "                        fill_opacity=0.7,           # 투명도    \n",
        "                        popup=name\n",
        "    ).add_to(cluster3_map)\n",
        "\n",
        "# 지도를 html 파일로 저장하기\n",
        "cluster3_map.save('./seoul_mschool_cluster3.html')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0    지역                               학교명  코드  유형  ...  \\\n",
            "0           0  성북구  서울대학교사범대학부설중학교.....       3  국립  ...   \n",
            "1           1  종로구  서울대학교사범대학부설여자중학교...     3  국립  ...   \n",
            "2           2  강남구           개원중학교                     3  공립  ...   \n",
            "3           3  강남구           개포중학교                     3  공립  ...   \n",
            "4           4  서초구           경원중학교                     3  공립  ...   \n",
            "\n",
            "         경도  location  code  type  day  \n",
            "0  127.038909        16     0     1    0  \n",
            "1  127.003857        22     0     1    0  \n",
            "2  127.071744         0     0     0    0  \n",
            "3  127.062201         0     0     0    0  \n",
            "4  127.008900        14     0     0    0  \n",
            "\n",
            "[5 rows x 25 columns]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}