{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "copyrighted-return",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developing-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding ,LSTM , Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "단어를 vectorization 하기 해야하는데.. 😒\n",
    "one hot encodding vs word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-toddler",
   "metadata": {},
   "source": [
    "#### Word embedding \n",
    "### Based on Frequency \n",
    "- CountVectorizer : 횟수\n",
    "- TfidfVectorizer : TF-IDF (Term Frequency X Inverse Document Frequency)  \n",
    " what i dont know :(LSA??),NPLM ?  \n",
    "### Based on Prediction \n",
    "- Word2Vec : Distributed hypothesis\n",
    "    - CBow \n",
    "    - Skip-gram\n",
    "- FastText\n",
    "- Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-northwest",
   "metadata": {},
   "source": [
    "##### keras Embedding Layer   \n",
    "Embedding(words set size,embedding vector size,input_dim=)  \n",
    "[ 13,3,5,7 ] - > [ [12,...,12] ,[23,...,44], [12,...,12] ,[23,...,44] ]  \n",
    "Q. [vector,vector,..] 도 인풋으로 들어갈수있는가 ? (pretrained 사용하고 또 임베딩 조질려고)  \n",
    "vector 는 안되고 정수 인코딩만 된단다 : [https://wikidocs.net/33793](https://wikidocs.net/33793)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-height",
   "metadata": {},
   "source": [
    "### Pre-trained Embedding Model\n",
    "korean\n",
    "- word2vec :\n",
    "    - [박규병님(2017년 마지막 업데이트,(30185개, 200차원))](https://github.com/Kyubyong/wordvectors)\n",
    "-     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "one hot encoding : 띠발 메모리 넘칠일있냐 \n",
    "word embedding : 그래 먼가 좀 비슷한 놈끼리 벡터 유사도도 갖고 해야지 모델이 이해를 할거 아이가 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-secretariat",
   "metadata": {},
   "source": [
    "### How to make Word Embedding ..\n",
    "띠부랄 좋아보이긴하는데 어떻게 임베딩 때리냐... ~~원핫인코가 쉽긴한데~~ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-affairs",
   "metadata": {},
   "source": [
    "### ways of word Embedding\n",
    "LSA, Word2Vec, FastText, Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-shakespeare",
   "metadata": {},
   "source": [
    "Time to Hunt Word2Vec , Doc2Vec  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-moment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
