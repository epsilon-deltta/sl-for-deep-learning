{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "engaged-brown",
   "metadata": {},
   "source": [
    "- lstm : [S.Hochreiter와 J.Schmidhuber가 1997년에 제안한 셀](https://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735#.WIxuWvErJnw)\n",
    "- GRU : [Learning Phrase Representations using RNN Encoder–Decoder\n",
    "for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078v3.pdf)                 - Seq2Seq    \n",
    "- Seq2Seq with Attention   \n",
    "- Transformer [Attention is All we need](https://arxiv.org/abs/1706.03762)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-vietnamese",
   "metadata": {},
   "source": [
    "### Initialization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-albert",
   "metadata": {},
   "source": [
    "- Zero prob : symmetry learning\n",
    "- Random  (Standard normal distribution) :  vanishing gradients problem, exploding gradients problem \n",
    "- Lecun : [Efficiency Backprop](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n",
    "- Xavier initialization(2010) :  [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "- He(2015) : [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)\n",
    "- LSUV(Layer-sequential unit-variance,2015) : [All you need is a good init](https://arxiv.org/abs/1511.06422)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-picture",
   "metadata": {},
   "source": [
    "### Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expected-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "- hold out\n",
    "- kFold\n",
    "- leave-p-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "- 데이터가 독립적이고 동일한 분포를 가진 경우\n",
    "KFold, RepeatedKFold, LeaveOneOut(LOO), LeavePOutLeaveOneOut(LPO)\n",
    "- 동일한 분포가 아닌 경우\n",
    "StratifiedKFold, RepeatedStratifiedKFold, StratifiedShuffleSplit\n",
    "- 그룹화된 데이터의 경우\n",
    "GroupKFold, LeaveOneGroupOut, LeavePGroupsOut, GroupShuffleSplit\n",
    "- 시계열 데이터의 경우\n",
    "TimeSeriesSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "= sklearn.model_selection\n",
    "== hyperparameter tunning\n",
    "GridSearchCV\n",
    "RandomizedSearchCV\n",
    "ParameterGrid\n",
    "\n",
    "== cross-validation\n",
    "\n",
    "train_test_split\n",
    "PredefinedSplit\n",
    "ShuffleSplit\n",
    "GroupShuffleSplit\n",
    "StratifiedShuffleSplit\n",
    "TimeSeriesSplit\n",
    "== kFold\n",
    "KFold\n",
    "GroupKFold\n",
    "StratifiedKFold\n",
    "RepeatedKFold\n",
    "RepeatedStratifiedKFold\n",
    "== leave-p-out\n",
    "LeaveOneOut\n",
    "LeaveOneGroupOut\n",
    "LeavePOut\n",
    "LeavePGroupsOut\n",
    "\n",
    "== validation index\n",
    "cross_val_predict\n",
    "cross_val_score # test accuracy \n",
    "cross_validate # fit_time,score_time, test_score,train_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-november",
   "metadata": {},
   "source": [
    "### NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-proxy",
   "metadata": {},
   "source": [
    "#### Word embedding  \n",
    "##### Based on Frequency \n",
    "- CountVectorizer : 횟수\n",
    "- TfidfVectorizer : TF-IDF (Term Frequency X Inverse Document Frequency)\n",
    "\n",
    "what i dont know :(LSA??),NPLM ?\n",
    "##### Based on Prediction \n",
    "- NPLM  : Bengio et al., A Neural Probabilistic Language Model, 2003\n",
    "- Word2Vec (google): Distributed hypothesis : Tomas Mikolov. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.\n",
    "    - CBow \n",
    "    - Skip-gram  \n",
    "- FastText (facebook)\n",
    "- Glove ()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-deadline",
   "metadata": {},
   "source": [
    "#### ABOUT word2vec : Birds of a feather flock together\n",
    "> __비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다__   \n",
    "distributed representation < distributional hypothesis\n",
    "- Cbow (Continuous Bag of Words)\n",
    "- Skip-gram (has better perpormance compared to CBOW)\n",
    "[problem](https://inspiringpeople.github.io/data%20analysis/word_embedding/)\n",
    "\n",
    "- [장단점](https://inspiringpeople.github.io/data%20analysis/word_embedding/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-supervisor",
   "metadata": {},
   "source": [
    "##### keras Embedding Layer   \n",
    "Embedding(words set size,embedding vector size,input_dim=)  \n",
    "[ 13,3,5,7 ] - > [ [12,...,12] ,[23,...,44], [12,...,12] ,[23,...,44] ]  \n",
    "Q. [vector,vector,..] 도 인풋으로 들어갈수있는가 ? (pretrained 사용하고 또 임베딩 조질려고)  \n",
    "vector 는 안되고 정수 인코딩만 된단다 : [https://wikidocs.net/33793](https://wikidocs.net/33793)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-bidding",
   "metadata": {},
   "source": [
    "#### Pre-trained Embedding Model\n",
    "korean\n",
    "- word2vec :\n",
    "    - [박규병님(2017년 마지막 업데이트,(30185개, 200차원))](https://github.com/Kyubyong/wordvectors)\n",
    "- FastText :\n",
    "    - [model](https://fasttext.cc/docs/en/crawl-vectors.html) , [.whl (pip install file.whl)(fasttext,it doesn't work well in windows)](https://www.lfd.uci.edu/~gohlke/pythonlibs/#fasttext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-wisconsin",
   "metadata": {},
   "source": [
    "### ways of word Embedding\n",
    "LSA, Word2Vec, FastText, Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-triple",
   "metadata": {},
   "source": [
    "### Sequantial Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-detroit",
   "metadata": {},
   "source": [
    "- lstm : [S.Hochreiter와 J.Schmidhuber가 1997년에 제안한 셀](https://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735#.WIxuWvErJnw)\n",
    "- GRU : [Learning Phrase Representations using RNN Encoder–Decoder\n",
    "for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078v3.pdf) (2014)               \n",
    "- Seq2Seq     \n",
    "- Seq2Seq with Attention   \n",
    "- Transformer [Attention is All we need](https://arxiv.org/abs/1706.03762)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-stability",
   "metadata": {},
   "source": [
    "### etc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-exposure",
   "metadata": {},
   "source": [
    "- residual connection : [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kera",
   "language": "python",
   "name": "kera"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
