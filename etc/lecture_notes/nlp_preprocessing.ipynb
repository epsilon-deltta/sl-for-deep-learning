{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tropical-carol",
   "metadata": {},
   "source": [
    "### TEXT PREPOCESSING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sealed-astronomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "center-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "enclosed-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "data = pd.read_csv('archive/spam.csv',encoding='latin1')\n",
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "saving-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "indirect-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "prf = df.profile_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "turkish-olive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128967360cd04d47b6559998d7fd5ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prf.to_file('./pr_report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "opposite-beaver",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\epsilon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-triangle",
   "metadata": {},
   "source": [
    "### Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "mathematical-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "testst=\"what?!?!? are you kidding me??!?! don't talk about that  \"\n",
    "testst=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "alert-paris",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['야', '라면', '먹고', '갈래', '?!?!', '싫은데', '!']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer as wpt\n",
    "wpt().tokenize(testst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "rough-demographic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['야', '라면', '먹고', '갈래', '?', '!', '?', '!', '싫은데', '!']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize as wt\n",
    "wt(testst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "unnecessary-accused",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['야', '라면', '먹고', '갈래', '싫은데']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence as ttws\n",
    "ttws(testst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "blind-brisbane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['야', '라면', '먹고', '갈래', '?', '!', '?', '!', '싫은데', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "# text=\"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "print(tokenizer.tokenize(testst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "greenhouse-pharmaceutical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "prescribed-taylor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['네 저는 현재 kaist.univ에서 ph.d 과정중입니다.', '딥 러닝 자연어 처리가 재미있기는 합니다....', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요.', '농담아니에요.', '이제 해보면 알걸요?']\n"
     ]
    }
   ],
   "source": [
    "# pip install kss\n",
    "import kss\n",
    "\n",
    "text='네 저는 현재 kaist.univ에서 ph.d 과정중입니다.  딥 러닝 자연어 처리가 재미있기는 합니다.... 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요. 이제 해보면 알걸요?'\n",
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "### binary classification\n",
    "### kss algorithm did binary classification ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-volleyball",
   "metadata": {},
   "source": [
    "### 형태소\n",
    "- 자립 형태소\n",
    "- 의존 형태소"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-combination",
   "metadata": {},
   "source": [
    "### part of speech tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "maritime-invention",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\epsilon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penn Treebank POG Tags에서 \n",
    "# PRP는 인칭 대명사, VBP는 동사, RB는 부사,\n",
    "# VBG는 현재부사, IN은 전치사, NNP는 고유 명사,\n",
    "# NNS는 복수형 명사, CC는 접속사, DT는 관사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "premium-hawaiian",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('동해물과', 'JJ'),\n",
       " ('백두산이', 'NNP'),\n",
       " ('마르고', 'NNP'),\n",
       " ('닳도록', 'NNP'),\n",
       " (',', ','),\n",
       " (',', ','),\n",
       " (',', ','),\n",
       " ('하느님이', 'NNP'),\n",
       " ('보호하사', 'NNP'),\n",
       " ('우리나라', 'NNP'),\n",
       " ('만세', 'NN'),\n",
       " ('!', '.'),\n",
       " ('!', '.'),\n",
       " ('!', '.'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# English\n",
    "from nltk.tokenize import word_tokenize\n",
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "text='동해물과 백두산이 마르고 닳도록 ,,, 하느님이 보호하사 우리나라 만세!!!.'\n",
    "# print(word_tokenize(text))\n",
    "from nltk.tag import pos_tag\n",
    "x=word_tokenize(text)\n",
    "pos_tag(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "disturbed-college",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
      "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "# Korean\n",
    "from konlpy.tag import Okt  \n",
    "okt=Okt()  \n",
    "print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']  \n",
    "print(okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  \n",
    "[('열심히','Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]  \n",
    "print(okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "noted-headset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n",
      "[('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n",
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma  \n",
    "kkma=Kkma()  \n",
    "print(kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']  \n",
    "print(kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  \n",
    "[('열심히','MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]  \n",
    "print(kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-president",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soynlp (unsupervised learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "moderate-villa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soynlp\n",
      "  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\epsilon\\dev\\anaconda\\envs\\kera\\lib\\site-packages (from soynlp) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\epsilon\\dev\\anaconda\\envs\\kera\\lib\\site-packages (from soynlp) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.12.1 in c:\\users\\epsilon\\dev\\anaconda\\envs\\kera\\lib\\site-packages (from soynlp) (1.19.5)\n",
      "Collecting psutil>=5.0.1\n",
      "  Downloading psutil-5.8.0-cp36-cp36m-win_amd64.whl (244 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\epsilon\\dev\\anaconda\\envs\\kera\\lib\\site-packages (from scikit-learn>=0.20.0->soynlp) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\epsilon\\dev\\anaconda\\envs\\kera\\lib\\site-packages (from scikit-learn>=0.20.0->soynlp) (1.0.0)\n",
      "Installing collected packages: psutil, soynlp\n",
      "Successfully installed psutil-5.8.0 soynlp-0.0.493\n"
     ]
    }
   ],
   "source": [
    "!pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "checked-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "important-thread",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] used default noun predictor; Sejong corpus predictor\n",
      "[Noun Extractor] used noun_predictor_sejong\n",
      "[Noun Extractor] All 2398 r features was loaded\n",
      "[Noun Extractor] scanning was done (L,R) has (0, 0) tokens\n",
      "[Noun Extractor] building L-R graph was done\n",
      "[Noun Extractor] 0 nouns are extracted\n"
     ]
    }
   ],
   "source": [
    "from soynlp.noun import LRNounExtractor\n",
    "sentences=['이것은 테스트입니다','강처럼 푸르고 바다처럼 넓은 그리고 캄캄한 우주']\n",
    "noun_extractor = LRNounExtractor()\n",
    "nouns = noun_extractor.train_extract(sentences) # list of str like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "competent-summer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used default noun predictor; Sejong corpus based logistic predictor\n",
      "c:/users/epsilon/dev/anaconda/envs/kera/lib/site-packages/soynlp\n",
      "local variable 'f' referenced before assignment\n",
      "local variable 'f' referenced before assignment\n",
      "scan vocabulary ... \n",
      "done (Lset, Rset, Eojeol) = (29, 16, 9)\n",
      "predicting noun score was done                                        \n",
      "before postprocessing 0\n",
      "_noun_scores_ 0\n",
      "checking hardrules ... doneNsubJ (떡볶+(이)), NVsubE (사기(당)+했다) ... done\n",
      "after postprocessing 0\n",
      "extracted 0 compounds from eojeols"
     ]
    }
   ],
   "source": [
    "from soynlp.noun import NewsNounExtractor\n",
    "noun_extractor = NewsNounExtractor()\n",
    "nouns = noun_extractor.train_extract(sentences) # list of str like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-insulation",
   "metadata": {},
   "source": [
    "##### 정규식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-velvet",
   "metadata": {},
   "source": [
    "### 불용어 , normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-gregory",
   "metadata": {},
   "source": [
    "### Stemming , Lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "global-gauge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\epsilon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "noticed-flavor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "n=WordNetLemmatizer()\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([n.lemmatize(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-screen",
   "metadata": {},
   "source": [
    "### 2.8 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-sport",
   "metadata": {},
   "source": [
    "#### Naver Movie Review Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "intermediate-vessel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "# urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intensive-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('../dataset/ratings_train.txt')\n",
    "test_data = pd.read_table('../dataset/ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "honey-vertical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146182, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['document'].nunique(), train_data['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chicken-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop_duplicates(subset=['document'], inplace=True) \n",
    "# document 열에서 중복인 내용이 있다면 중복 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "train_data = train_data.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "national-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "test_data = test_data.dropna(how='any') # Null 값 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "touched-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "minor-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from konlpy.tag import Okt as okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "narrow-claim",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "morphs() missing 1 required positional argument: 'phrase'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-4c3c0c55ea79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'document'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtemp_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtemp_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mokt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 토큰화\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtemp_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_X\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# 불용어 제거\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: morphs() missing 1 required positional argument: 'phrase'"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = []\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for sentence in test_data['document']:\n",
    "    temp_X = []\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = total_cnt - rare_cnt + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 30\n",
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "    score = float(loaded_model.predict(pad_new)) # 예측\n",
    "    if(score > 0.5):\n",
    "    print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "    else:\n",
    "    print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-sunrise",
   "metadata": {},
   "source": [
    "### Naver Shopping Review Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-vancouver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "# %cd Mecab-ko-for-Google-Colab\n",
    "# !bash install_mecab-ko_on_colab190912.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "comic-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "completed-prime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../dataset/ratings_total.txt', <http.client.HTTPMessage at 0x16b1e5a87b8>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt\", filename=\"../dataset/ratings_total.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "flush-domain",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pd.read_table('../dataset/ratings_total.txt', names=['ratings', 'reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eleven-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data['label'] = np.select([total_data.ratings > 3], [1], default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adult-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data.drop_duplicates(subset=['reviews'], inplace=True) # reviews 열에서 중복인 내용이 있다면 중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "mobile-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(total_data, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "registered-venezuela",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\epsilon\\dev\\anaconda\\envs\\kera\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "c:\\users\\epsilon\\dev\\anaconda\\envs\\kera\\lib\\site-packages\\pandas\\core\\series.py:4582: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  method=method,\n"
     ]
    }
   ],
   "source": [
    "# 한글과 공백을 제외하고 모두 제거\n",
    "train_data['reviews'] = train_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "train_data['reviews'].replace('', np.nan, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "union-possession",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\epsilon\\dev\\anaconda\\envs\\kera\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\epsilon\\dev\\anaconda\\envs\\kera\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "test_data.drop_duplicates(subset = ['reviews'], inplace=True) # 중복 제거\n",
    "test_data['reviews'] = test_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "test_data['reviews'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "test_data = test_data.dropna(how='any') # Null 값 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "documentary-formation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['와', '이런', '것', '도', '상품', '이', '라고', '차라리', '내', '가', '만드', '는', '게', '나을', '뻔']\n"
     ]
    }
   ],
   "source": [
    "import eunjeon as ej\n",
    "mecab = ej.Mecab()\n",
    "print(mecab.morphs('와 이런 것도 상품이라고 차라리 내가 만드는 게 나을 뻔'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "linear-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "lesbian-radiation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\epsilon\\dev\\anaconda\\envs\\kera\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\users\\epsilon\\dev\\anaconda\\envs\\kera\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_data['tokenized'] = train_data['reviews'].apply(mecab.morphs)\n",
    "train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "powerful-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['tokenized'] = test_data['reviews'].apply(mecab.morphs)\n",
    "test_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "greenhouse-violation",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = np.hstack(train_data[train_data.label == 0]['tokenized'].values)\n",
    "positive_words = np.hstack(train_data[train_data.label == 1]['tokenized'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "standard-civilian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('네요', 31823), ('는데', 20095), ('안', 19748), ('어요', 14869), ('있', 13200), ('너무', 13056), ('했', 11796), ('좋', 9797), ('배송', 9623), ('같', 8995), ('거', 8904), ('어', 8896), ('구매', 8884), ('없', 8695), ('아요', 8636), ('습니다', 8436), ('그냥', 8353), ('되', 8350), ('잘', 8029), ('않', 7986)]\n",
      "[('좋', 39365), ('아요', 21151), ('네요', 19906), ('어요', 18672), ('잘', 18612), ('구매', 16175), ('습니다', 13322), ('있', 12380), ('배송', 12263), ('는데', 11579), ('합니다', 9850), ('했', 9806), ('먹', 9453), ('재', 9252), ('너무', 8398), ('같', 7867), ('만족', 7225), ('거', 6485), ('기', 6341), ('어', 6317)]\n"
     ]
    }
   ],
   "source": [
    "negative_word_count = Counter(negative_words)\n",
    "print(negative_word_count.most_common(20))\n",
    "positive_word_count = Counter(positive_words)\n",
    "print(positive_word_count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "outstanding-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data['tokenized'].values\n",
    "y_train = train_data['label'].values\n",
    "X_test= test_data['tokenized'].values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "brief-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "exceptional-february",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 39741\n",
      "등장 빈도가 1번 이하인 희귀 단어의 수: 18056\n",
      "단어 집합에서 희귀 단어의 비율: 45.43418635665937\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 0.7869324583053495\n"
     ]
    }
   ],
   "source": [
    "threshold = 2\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "prerequisite-disorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 21687\n"
     ]
    }
   ],
   "source": [
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "loved-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cosmetic-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 80\n",
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-template",
   "metadata": {},
   "source": [
    "##### train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "integral-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, GRU , Bidirectional ,LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "palestinian-utility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - ETA: 0s - loss: 0.3257 - acc: 0.8710\n",
      "Epoch 00001: val_acc improved from -inf to 0.90222, saving model to ./model/best_model.h5\n",
      "200/200 [==============================] - 238s 1s/step - loss: 0.3257 - acc: 0.8710 - val_loss: 0.2607 - val_acc: 0.9022\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('./model/best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=1, callbacks=[es, mc], batch_size=600, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    new_sentence = mecab.morphs(new_sentence) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "    score = float(loaded_model.predict(pad_new)) # 예측\n",
    "    if(score > 0.5):\n",
    "    print(\"{:.2f}% 확률로 긍정 리뷰입니다.\".format(score * 100))\n",
    "    else:\n",
    "    print(\"{:.2f}% 확률로 부정 리뷰입니다.\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict('이 상품 진짜 좋아요... 저는 강추합니다. 대박')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
