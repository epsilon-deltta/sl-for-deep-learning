{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import random\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "from sklearn import linear_model\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.regression.quantile_regression import QuantReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout , Conv2D ,MaxPooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data 와 test data로 구분(7:3 비율) X,y : DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores=[]\n",
    "\n",
    "for train_idx, val_idx in k_fold.split(X):\n",
    "#     print(train_idx,\" : \",val_idx)\n",
    "\n",
    "    # split train, validation set\n",
    "    xtr = X.iloc[train_idx]\n",
    "    ytr = y.iloc[train_idx]\n",
    "    xte = X.iloc[val_idx]\n",
    "    yte = y.iloc[val_idx]\n",
    "    lr = lrg()\n",
    "    lr.fit(xtr,ytr)\n",
    "    score = lr.score(xte,yte)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr0 = LinearRegression()\n",
    "lr0.fit(x_train,y_train)\n",
    "# lr0.coef_ , lr0.intercept_\n",
    "sco = lr0.score(x_test,y_test)\n",
    "lr0.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### time check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time as tt\n",
    "iterate_n = 10\n",
    "first = tt()\n",
    "for x in range(iterate_n) :\n",
    "#     train_err[train_err['user_id'].isin(strange_user_id)]\n",
    "duration = tt() -first\n",
    "print(\"duration : \" ,duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### google search api "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import json\n",
    "my_api_key = \"api key\"\n",
    "my_cse_id = \"custom search engine id\"  \n",
    "def google_search(search_term, api_key, cse_id, **kwargs):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()\n",
    "    return res\n",
    "#구글로 검색하기\n",
    "search_result = google_search(\"Python Google Custom Search\", my_api_key, my_cse_id)\n",
    "\n",
    "with open('search_result_all.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(search_result, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scaling & oneHot Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫인코딩(더미 변수)\n",
    "from sklearn import preprocessing    \n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()     # label encoder 생성\n",
    "onehot_encoder = preprocessing.OneHotEncoder()   # one hot encoder 생성\n",
    "\n",
    "onehot_location = label_encoder.fit_transform(df['지역'])\n",
    "onehot_code = label_encoder.fit_transform(df['코드'])\n",
    "onehot_type = label_encoder.fit_transform(df['유형'])\n",
    "onehot_day = label_encoder.fit_transform(df['주야'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### decission Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "xtr , xte,ytr,yte = tts(x,y,random_state=10,test_size=0.3)\n",
    "from sklearn.tree import DecisionTreeClassifier as dtc\n",
    "dtcr =dtc(criterion='entropy', max_depth=5)\n",
    "dtcr.fit(xtr,ytr)\n",
    "\n",
    "from sklearn import metrics \n",
    "tree_matrix = metrics.confusion_matrix(y_test, y_hat)  \n",
    "print(tree_matrix)\n",
    "print('\\n')\n",
    "\n",
    "# 모형 성능 평가 - 평가지표 계산\n",
    "tree_report = metrics.classification_report(y_test, y_hat)            \n",
    "print(tree_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pca "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "comp_n = 2\n",
    "pca = PCA(n_components=comp_n) # 주성분을 몇개로 할지 결정\n",
    "cols = []\n",
    "for x in range(comp_n):\n",
    "    cols.append('comp_%d'%x)\n",
    "printcipalComponents = pca.fit_transform(X)\n",
    "principalDf = pd.DataFrame(data=printcipalComponents, columns = cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "colors = {-1:'gray', 0:'coral', 1:'blue', 2:'green', 3:'red', 4:'purple', \n",
    "          5:'orange', 6:'brown', 7:'brick', 8:'yellow', 9:'magenta', 10:'cyan',11:'pink'}\n",
    "\n",
    "df = itaca\n",
    "cluster2_map = folium.Map(location=[37.55,126.98], tiles='Stamen Terrain', \n",
    "                        zoom_start=12)\n",
    "\n",
    "for name, lat, lng in zip(df.상호명, df.위도, df.경도):  \n",
    "    folium.CircleMarker([lat, lng],\n",
    "                        radius=5,                   # 원의 반지름\n",
    "                        color=colors[1],         # 원의 둘레 색상\n",
    "                        fill=True,\n",
    "                        fill_color=colors[1],    # 원을 채우는 색\n",
    "                        fill_opacity=0.7,           # 투명도    \n",
    "                        popup=name\n",
    "    ).add_to(cluster2_map)\n",
    "cluster2_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cluster\n",
    "colors = {-1:'gray', 0:'coral', 1:'blue', 2:'green', 3:'red', 4:'purple', \n",
    "          5:'orange', 6:'brown', 7:'brick', 8:'yellow', 9:'magenta', 10:'cyan',11:'pink'}\n",
    "\n",
    "columns_list =['경도','위도']\n",
    "X3 = itaca.loc[:, columns_list]\n",
    "x = preprocessing.StandardScaler().fit(X3).transform(X3)\n",
    "\n",
    "# dbm = cluster.DBSCAN()#eps=0.2, min_samples=5\n",
    "dbm = cluster.KMeans(init='k-means++', n_clusters=5, n_init=10)\n",
    "dbm.fit(x)  \n",
    "\n",
    "itaca['cluster'] = dbm.labels_   \n",
    "\n",
    "cluster3_map = folium.Map(location=[37.55,126.98], tiles='Stamen Terrain', \n",
    "                        zoom_start=12)\n",
    "\n",
    "for name, lat, lng, clus in zip(itaca.상호명, itaca.위도, itaca.경도, itaca.cluster):  \n",
    "    folium.CircleMarker([lat, lng],\n",
    "                        radius=5,                   # 원의 반지름\n",
    "                        color=colors[clus],         # 원의 둘레 색상\n",
    "                        fill=True,\n",
    "                        fill_color=colors[clus],    # 원을 채우는 색\n",
    "                        fill_opacity=0.7,           # 투명도    \n",
    "                        popup=name\n",
    "    ).add_to(cluster3_map)\n",
    "cluster3_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cluster\n",
    "colors = {-1:'gray', 0:'coral', 1:'blue', 2:'green', 3:'red', 4:'purple', \n",
    "          5:'orange', 6:'brown', 7:'brick', 8:'yellow', 9:'magenta', 10:'cyan',11:'pink'}\n",
    "\n",
    "columns_list =['경도','위도']\n",
    "X3 = itaca.loc[:, columns_list]\n",
    "x = preprocessing.StandardScaler().fit(X3).transform(X3)\n",
    "\n",
    "# dbm = cluster.DBSCAN()#eps=0.2, min_samples=5\n",
    "dbm = cluster.KMeans(init='k-means++', n_clusters=5, n_init=10)\n",
    "dbm.fit(x)  \n",
    "\n",
    "itaca['cluster'] = dbm.labels_   \n",
    "\n",
    "cluster3_map = folium.Map(location=[37.55,126.98], tiles='Stamen Terrain', \n",
    "                        zoom_start=12)\n",
    "\n",
    "for name, lat, lng, clus in zip(itaca.상호명, itaca.위도, itaca.경도, itaca.cluster):  \n",
    "    folium.CircleMarker([lat, lng],\n",
    "                        radius=5,                   # 원의 반지름\n",
    "                        color=colors[clus],         # 원의 둘레 색상\n",
    "                        fill=True,\n",
    "                        fill_color=colors[clus],    # 원을 채우는 색\n",
    "                        fill_opacity=0.7,           # 투명도    \n",
    "                        popup=name\n",
    "    ).add_to(cluster3_map)\n",
    "cluster3_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "# 10개의 파일로 쪼갬\n",
    "n_fold = 2\n",
    "skf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "\n",
    "\n",
    "# 모델의 설정, 컴파일, 실행\n",
    "for train, test in skf.split(X, Y):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=60, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X[train], Y[train], epochs=100, batch_size=5)\n",
    "    k_accuracy = \"%.4f\" % (model.evaluate(X[test], Y[test])[1])\n",
    "    accuracy.append(k_accuracy)\n",
    "\n",
    "model.save('sonar_model.h5')\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "exsonar = load_model('sonar_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "#from sklearn.ensemble import IsolationForest\n",
    "#from sklearn.svm import OneClassSVM\n",
    "\n",
    "outlier = LocalOutlierFactor()\n",
    "#outlier = IsolationForest()\n",
    "#outlier = OneClassSVM()\n",
    "\n",
    "y_predict = outlier.fit_predict(df_train.drop(['score'], axis=1))\n",
    "# print(y_predict) #[ 1  1  1  1  1  1  1 -1  1  1  1]\n",
    "df_train['outlier'] = y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='binary_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "# 모델 저장 폴더 설정\n",
    "MODEL_DIR = './model0/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "   os.mkdir(MODEL_DIR)\n",
    "\n",
    "# 모델 저장 조건 설정\n",
    "modelpath=MODEL_DIR+\"{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
    "# 모델 실행 및 저장\n",
    "history = model.fit(X, Y, validation_split=0.2, epochs=200, batch_size=200, verbose=0, callbacks=[checkpointer,early_stopping_callback])\n",
    "\n",
    "# y_vloss에 테스트셋으로 실험 결과의 오차 값을 저장\n",
    "y_vloss=history.history['val_loss']\n",
    "\n",
    "# y_acc 에 학습 셋으로 측정한 정확도의 값을 저장\n",
    "y_acc=history.history['val_accuracy']\n",
    "\n",
    "# x값을 지정하고 정확도를 파란색으로, 오차를 빨간색으로 표시\n",
    "x_len = numpy.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=3)\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# early stop\n",
    "# 결과 출력\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 코드 내부에 한글을 사용가능 하게 해주는 부분입니다.\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seed 값 설정\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "# 학습셋, 테스트셋 지정하기\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=5000)\n",
    "\n",
    "# 데이터 전처리\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=100)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=100)\n",
    "\n",
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(64, 5, padding='valid', activation='relu',strides=1))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(55))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# 모델의 컴파일\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델의 실행\n",
    "history = model.fit(x_train, y_train, batch_size=100, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "# 테스트 정확도 출력\n",
    "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(x_test, y_test)[1]))\n",
    "\n",
    "# 테스트 셋의 오차\n",
    "y_vloss = history.history['val_loss']\n",
    "\n",
    "# 학습셋의 오차\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "# 그래프로 표현\n",
    "x_len = numpy.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, marker='.', c=\"red\", label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c=\"blue\", label='Trainset_loss')\n",
    "\n",
    "# 그래프에 그리드를 주고 레이블을 표시\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "# 모델 저장 폴더 설정\n",
    "MODEL_DIR = './model0/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "   os.mkdir(MODEL_DIR)\n",
    "\n",
    "# 모델 저장 조건 설정\n",
    "modelpath=MODEL_DIR+\"{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
    "# 모델 실행 및 저장\n",
    "history = model.fit(X, Y, validation_split=0.2, epochs=200, batch_size=200, verbose=0, callbacks=[checkpointer,early_stopping_callback])\n",
    "\n",
    "# y_vloss에 테스트셋으로 실험 결과의 오차 값을 저장\n",
    "y_vloss=history.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pre-trained word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word2vec_file_path = './model/kow2v/ko.bin'\n",
    "ko_model = gensim.models.Word2Vec.load(word2vec_file_path)\n",
    "# ko_model.wv.most_similar(\"강아지\")\n",
    "\n",
    "vector_size = len(ko_model.wv['아무']) #200\n",
    "vocab_size  = len(index_to_word) #10000\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, vector_size))\n",
    "np.shape(embedding_matrix)\n",
    "\n",
    "def get_vector(word):\n",
    "    if word in ko_model.wv:\n",
    "        return ko_model.wv[word]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "num = 0\n",
    "not_used = []\n",
    "for word, i in word_to_index.items(): # 훈련 데이터의 단어 집합에서 단어와 정수 인덱스를 1개씩 꺼내온다.\n",
    "    temp = get_vector(word) # 단어(key) 해당되는 임베딩 벡터의 300개의 값(value)를 임시 변수에 저장\n",
    "    if temp is not None: # 만약 None이 아니라면 임베딩 벡터의 값을 리턴받은 것이므로\n",
    "        embedding_matrix[i] = temp # 해당 단어 위치의 행에 벡터의 값을 저장한다.\n",
    "    else :\n",
    "        not_used.append(word)\n",
    "        num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "e = Embedding(vocab_size, vector_size, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
    "model1.add(e)\n",
    "# model1.add(Flatten())\n",
    "# model.add(Dropout(0.5))\n",
    "model1.add(Conv1D(64, 5, padding='valid', activation='relu',strides=1))\n",
    "model1.add(MaxPooling1D(pool_size=4))\n",
    "model1.add(LSTM(55))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### nlp preprocessing enssential process\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "token = Tokenizer()             # token.word_index , token.document_count , token.word_docs, token.word_index\n",
    "token.fit_on_texts(docs)   \n",
    "token.texts_to_sequences(docs)\n",
    "padded_x = pad_sequences(x, 4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ways of drawing matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))   \n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.subplot(2,2,1)\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, constrained_layout=True, sharey=True)\n",
    "axs[0].plot(t1, f(t1), 'o', t2, f(t2), '-')\n",
    "axs[1].plot(t3, np.cos(2*np.pi*t3), '--')\n",
    "axs[1].grid(True)  \n",
    "\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.title('jax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Current time (Year ,month,day, Hour, Minute, Second ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    " \n",
    "now = datetime.datetime.now()\n",
    "print(now)          # 2015-04-19 12:11:32.669083 \n",
    "nowDate = now.strftime('%Y-%m-%d')\n",
    "print(nowDate)      # 2015-04-19\n",
    "nowTime = now.strftime('%H:%M:%S')\n",
    "print(nowTime)      # 12:11:32 \n",
    "nowDatetime = now.strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 차이 \n",
    "\n",
    "#1.일,월,주\n",
    "import datetime\n",
    "a=datetime.datetime.now()-datetime.timedelta(days=1)#하루전\n",
    "a=datetime.datetime.now()-datetime.timedelta(hours=1)#한시간전\n",
    "a=datetime.datetime.now()-datetime.timedelta(weeks=1)#일주일전\n",
    "#2.달,년\n",
    "import dateutil.relativedelta import relativedelta\n",
    "a=datetime.datetime.now()-relativedelta(months=1)#한달전"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### crawling (seleniem) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome(r'C:\\Users\\epsilon\\dev\\chromedriver_win32\\chromedriver.exe') #또는 chromedriver.exe\n",
    "\n",
    "driver.get(url)\n",
    "content = driver.find_element_by_xpath(r'//*[@id=\"powerballLogBox\"]/tbody[2]')\n",
    "docs = content.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### missing values by columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Missing Values\n",
    "# Function to calculate missing values by column\n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =================================\n",
    "# ================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  onehotencoder\n",
    "from sklearn import preprocessing    \n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()     # label encoder 생성\n",
    "onehot_encoder = preprocessing.OneHotEncoder()   # one hot encoder 생성\n",
    "\n",
    "label_encoder.fit(sse)\n",
    "se  = label_encoder.transform(sse)\n",
    "# se = label_encoder.fit_transform(df['pclass'])\n",
    "from keras.utils import np_utils\n",
    "Y_train = np_utils.to_categorical(se, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'cp949' codec can't decode byte 0x80 in position 0: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-0b19e5e63d78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mword_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2vec_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cp949'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mvector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'사랑'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\epsilon\\dev\\anaconda\\envs\\kera\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1547\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1548\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1549\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\epsilon\\dev\\anaconda\\envs\\kera\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, binary_chunk_size)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\epsilon\\dev\\anaconda\\envs\\kera\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36many2unicode\u001b[1;34m(text, encoding, errors)\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'cp949' codec can't decode byte 0x80 in position 0: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['사랑']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model load and save\n",
    "from gensim.models import KeyedVectors\n",
    "model.wv.save_word2vec_format('eng_w2v') # 모델 저장\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\") # 모델 로드"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
