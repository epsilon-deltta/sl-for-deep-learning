{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "separated-vintage",
   "metadata": {},
   "source": [
    "#### 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "olive-jackson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "session = tf.Session()\n",
    "print(session.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "soviet-mouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "z = x + y\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "values = {x: 5.0, y: 4.0}\n",
    "\n",
    "result = session.run([z], values)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "anticipated-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(tf.float32, name='x')\n",
    "y = tf.placeholder(tf.float32, name='y')\n",
    "z = tf.add(x, y, name='sum')\n",
    "session = tf.Session()\n",
    "summary_writer = tf.summary.FileWriter('/tmp/1', session.graph)\n",
    "summary_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-craps",
   "metadata": {},
   "source": [
    "#### 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developing-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "fully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hawaiian-whale",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "input_size = 784\n",
    "no_classes = 10\n",
    "batch_size = 100\n",
    "total_batches = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "flying-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = tf.placeholder(tf.float32, shape=[None, input_size])\n",
    "y_input = tf.placeholder(tf.float32, shape=[None, no_classes])\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([input_size, no_classes]))\n",
    "bias = tf.Variable(tf.random_normal([no_classes]))\n",
    "\n",
    "logits = tf.matmul(x_input, weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "printable-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_input, logits=logits)\n",
    "loss_operation = tf.reduce_mean(softmax_cross_entropy)\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sunset-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "emotional-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_no in range(total_batches):\n",
    "    mnist_batch = mnist_data.train.next_batch(batch_size)\n",
    "    train_images, train_labels = mnist_batch[0], mnist_batch[1]\n",
    "    _, loss_value = session.run([optimiser, loss_operation], feed_dict={x_input: train_images,\n",
    "                                                                        y_input: train_labels})\n",
    "    print(loss_value)\n",
    "\n",
    "predictions = tf.argmax(logits, 1)\n",
    "correct_predictions = tf.equal(predictions, tf.argmax(y_input, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "test_images, test_labels = mnist_data.test.images, mnist_data.test.labels\n",
    "accuracy_value = session.run(accuracy_operation, feed_dict={x_input: test_images,\n",
    "                                                            y_input: test_labels})\n",
    "                                                            \n",
    "print('Accuracy : ', accuracy_value)\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-delta",
   "metadata": {},
   "source": [
    "### convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "robust-continent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "input_size = 784\n",
    "no_classes = 10\n",
    "batch_size = 100\n",
    "total_batches = 200\n",
    "\n",
    "x_input = tf.placeholder(tf.float32, shape=[None, input_size])\n",
    "y_input = tf.placeholder(tf.float32, shape=[None, no_classes])\n",
    "\n",
    "\n",
    "def add_variable_summary(tf_variable, summary_name):\n",
    "    with tf.name_scope(summary_name + '_summary'):\n",
    "        mean = tf.reduce_mean(tf_variable)\n",
    "        tf.summary.scalar('Mean', mean)\n",
    "        with tf.name_scope('standard_deviation'):\n",
    "            standard_deviation = tf.sqrt(tf.reduce_mean(\n",
    "                tf.square(tf_variable - mean)))\n",
    "        tf.summary.scalar('StandardDeviation', standard_deviation)\n",
    "        tf.summary.scalar('Maximum', tf.reduce_max(tf_variable))\n",
    "        tf.summary.scalar('Minimum', tf.reduce_min(tf_variable))\n",
    "        tf.summary.histogram('Histogram', tf_variable)\n",
    "\n",
    "\n",
    "x_input_reshape = tf.reshape(x_input, [-1, 28, 28, 1],\n",
    "                             name='input_reshape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "presidential-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_layer(input_layer, filters, kernel_size=[3, 3],\n",
    "                      activation=tf.nn.relu):\n",
    "    layer = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        activation=activation\n",
    "    )\n",
    "    add_variable_summary(layer, 'convolution')\n",
    "    return layer\n",
    "\n",
    "\n",
    "def pooling_layer(input_layer, pool_size=[2, 2], strides=2):\n",
    "    layer = tf.layers.max_pooling2d(\n",
    "        inputs=input_layer,\n",
    "        pool_size=pool_size,\n",
    "        strides=strides\n",
    "    )\n",
    "    add_variable_summary(layer, 'pooling')\n",
    "    return layer\n",
    "\n",
    "\n",
    "def dense_layer(input_layer, units, activation=tf.nn.relu):\n",
    "    layer = tf.layers.dense(\n",
    "        inputs=input_layer,\n",
    "        units=units,\n",
    "        activation=activation\n",
    "    )\n",
    "    add_variable_summary(layer, 'dense')\n",
    "    return layer\n",
    "\n",
    "\n",
    "convolution_layer_1 = convolution_layer(x_input_reshape, 64)\n",
    "pooling_layer_1 = pooling_layer(convolution_layer_1)\n",
    "convolution_layer_2 = convolution_layer(pooling_layer_1, 128)\n",
    "pooling_layer_2 = pooling_layer(convolution_layer_2)\n",
    "flattened_pool = tf.reshape(pooling_layer_2, [-1, 5 * 5 * 128],\n",
    "                            name='flattened_pool')\n",
    "dense_layer_bottleneck = dense_layer(flattened_pool, 1024)\n",
    "\n",
    "dropout_bool = tf.placeholder(tf.bool)\n",
    "dropout_layer = tf.layers.dropout(\n",
    "        inputs=dense_layer_bottleneck,\n",
    "        rate=0.4,\n",
    "        training=dropout_bool\n",
    "    )\n",
    "logits = dense_layer(dropout_layer, no_classes)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=y_input, logits=logits)\n",
    "    loss_operation = tf.reduce_mean(softmax_cross_entropy, name='loss')\n",
    "    tf.summary.scalar('loss', loss_operation)\n",
    "\n",
    "with tf.name_scope('optimiser'):\n",
    "    optimiser = tf.train.AdamOptimizer().minimize(loss_operation)\n",
    "\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        predictions = tf.argmax(logits, 1)\n",
    "        correct_predictions = tf.equal(predictions, tf.argmax(y_input, 1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy_operation = tf.reduce_mean(\n",
    "            tf.cast(correct_predictions, tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy_operation)\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "merged_summary_operation = tf.summary.merge_all()\n",
    "train_summary_writer = tf.summary.FileWriter('/tmp/train', session.graph)\n",
    "test_summary_writer = tf.summary.FileWriter('/tmp/test')\n",
    "\n",
    "test_images, test_labels = mnist_data.test.images, mnist_data.test.labels\n",
    "\n",
    "for batch_no in range(total_batches):\n",
    "    mnist_batch = mnist_data.train.next_batch(batch_size)\n",
    "    train_images, train_labels = mnist_batch[0], mnist_batch[1]\n",
    "    _, merged_summary = session.run([optimiser, merged_summary_operation],\n",
    "                                    feed_dict={\n",
    "        x_input: train_images,\n",
    "        y_input: train_labels,\n",
    "        dropout_bool: True\n",
    "    })\n",
    "\n",
    "    train_summary_writer.add_summary(merged_summary, batch_no)\n",
    "    if batch_no % 10 == 0:\n",
    "        merged_summary, _ = session.run([merged_summary_operation,\n",
    "                                         accuracy_operation], feed_dict={\n",
    "            x_input: test_images,\n",
    "            y_input: test_labels,\n",
    "            dropout_bool: False\n",
    "        })\n",
    "        test_summary_writer.add_summary(merged_summary, batch_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-stuart",
   "metadata": {},
   "source": [
    "### cnn with KERAS 😀😀 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 128\n",
    "no_classes = 10\n",
    "epochs = 50\n",
    "image_height, image_width = 28, 28\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], image_height, image_width, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], image_height, image_width, 1)\n",
    "input_shape = (image_height, image_width, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, no_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, no_classes)\n",
    "\n",
    "\n",
    "def simple_cnn(input_shape):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu',\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=1024, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Dense(units=no_classes, activation='softmax'))\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                  optimizer=tf.keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "simple_cnn_model = simple_cnn(input_shape)\n",
    "\n",
    "simple_cnn_model.fit(x_train, y_train, batch_size, epochs, (x_test, y_test))\n",
    "train_loss, train_accuracy = simple_cnn_model.evaluate(\n",
    "    x_train, y_train, verbose=0)\n",
    "print('Train data loss:', train_loss)\n",
    "print('Train data accuracy:', train_accuracy)\n",
    "\n",
    "test_loss, test_accuracy = simple_cnn_model.evaluate(\n",
    "    x_test, y_test, verbose=0)\n",
    "print('Test data loss:', test_loss)\n",
    "print('Test data accuracy:', test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1v",
   "language": "python",
   "name": "tfv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
