{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "blind-wallace",
   "metadata": {},
   "source": [
    "## gray to color (CycleGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "forced-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "downtown-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow 1.x 버전 호환 관련 설정\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unsigned-passion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting utils\n",
      "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
      "Installing collected packages: utils\n",
      "Successfully installed utils-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "nonprofit-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Activation, Dense, Input, concatenate\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Conv2DTranspose, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.layers import Reshape, Dropout, ZeroPadding2D, Add, add\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import other_utils\n",
    "import instancenormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "distant-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator_unet(input_shape, output_shape, kernel_size, gen_n_filters):\n",
    "    def encoder(layer_input, filters, strides=2, activation='relu'):\n",
    "        conv = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding='same')\n",
    "        d = instancenormalization.InstanceNormalization(axis=-1, center=False, scale=False)(layer_input)\n",
    "        if activation == 'relu':\n",
    "            d = Activation('relu')(d)\n",
    "        else:\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "        d = conv(d)\n",
    "        return d\n",
    "    def decoder(layer_input, concate_input, filters, activation='relu'):\n",
    "        conv = Conv2DTranspose(filters, kernel_size, strides=2, padding='same')\n",
    "        u = instancenormalization.InstanceNormalization(axis=-1, center=False, scale=False)(layer_input)\n",
    "        if activation == 'relu':\n",
    "            u = Activation('relu')(u)\n",
    "        else:\n",
    "            u = LeakyReLU(alpha=0.2)(u)\n",
    "        u = conv(u)\n",
    "        u = concatenate([u, concate_input])\n",
    "        return u\n",
    "    img = Input(shape=input_shape)\n",
    "    channels = int(output_shape[-1])\n",
    "    \n",
    "    d1 = encoder(img, gen_n_filters, strides=1, activation='notrelu')\n",
    "    d2 = encoder(d1, gen_n_filters*2, activation='notrelu')\n",
    "    d3 = encoder(d2, gen_n_filters*4, activation='notrelu')\n",
    "    d4 = encoder(d3, gen_n_filters*8, activation='notrelu')\n",
    "    u1 = decoder(d4, d3, gen_n_filters*4)\n",
    "    u2 = decoder(u1, d2, gen_n_filters*2)\n",
    "    u3 = decoder(u2, d1, gen_n_filters)\n",
    "    \n",
    "    output = Conv2DTranspose(channels, kernel_size=kernel_size, strides=1, padding='same', activation='relu')(u3)\n",
    "    generator = Model(img, output)\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "completed-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator_resnet(input_shape, kernel_size, gen_n_filters, output_shape=None):\n",
    "    def conv7s1(layer_input, filters, final):\n",
    "        y = Conv2D(filters, kernel_size=(7,7), strides=1, padding='same')(layer_input)\n",
    "        if final:\n",
    "            y = Activation('tanh')(y)\n",
    "        else:\n",
    "            y = instancenormalization.InstanceNormalization(axis=-1, center=False, scale=False)(y)\n",
    "        return y\n",
    "    def encoder(layer_input, filters):\n",
    "        y = Conv2D(filters, kernel_size=Kernel_size, strides=2, padding='same')(layer_input)\n",
    "        y = instancenormalization.InstanceNormalization(axis=-1, center=False, scale=False)(y)\n",
    "        y = Activation('relu')(y)\n",
    "        return y\n",
    "    def residual(layer_input, filters):\n",
    "        short_cut = layer_input\n",
    "        y = Conv2D(filters, kernel_size=kernel_size, strides=1, padding='same')(layer_input)\n",
    "        y = instancenormalization.InstanceNormalization(axis=-1, center=False, scale=False)(y)\n",
    "        y = Activation('relu')(y)\n",
    "        y = Conv2D(filters, kernel_size=kernel_size, strides=1, padding='same')(y)\n",
    "        y = instancenormalization.InstanceNormalization(axis=-1, center=False, scale=False)(y)\n",
    "        return add([short_cut, y])\n",
    "    def decoder(layer_input, filters):\n",
    "        y = Conv2DTranspose(filters, kernel_size=kernel_size, strides=2, padding='same')(layer_input)\n",
    "        y = instancenormalization.InstanceNormalization(axis=-1, center=False, scale=False)(y)\n",
    "        y = Activation('relu')(y)\n",
    "        return y\n",
    "    img = Input(shape=input_shape)\n",
    "    channels = int(output_shape[-1])\n",
    "    y = conv7s1(img, gen_n_filters, False)\n",
    "    y = encoder(y, gen_n_filters*2)\n",
    "    y = encoder(y, gen_n_filters*4)\n",
    "    y = residual(y, gen_n_filters*4)\n",
    "    y = residual(y, gen_n_filters*4)\n",
    "    y = residual(y, gen_n_filters*4)\n",
    "    y = residual(y, gen_n_filters*4)\n",
    "    y = residual(y, gen_n_filters*4)\n",
    "    y = residual(y, gen_n_filters*4)\n",
    "    y = residual(y, gen_n_filters*4)\n",
    "    y = residual(y, gen_n_filters*4)\n",
    "    y = residual(y, gen_n_filters*4)\n",
    "    y = decoder(y, gen_n_filters*2)\n",
    "    y = decoder(y, gen_n_filters)\n",
    "    output = conv7s1(y, channels, True)\n",
    "    generator = Model(img, output)\n",
    "    return generatoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "secret-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(input_shape, kernel_size, dis_n_filters, patchgan=False):\n",
    "    def conv(layer_input, filters, strides=2):\n",
    "        conv = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding='same')\n",
    "        y = LeakyReLU(0.2)(layer_input)\n",
    "        y = conv(y)\n",
    "        return y\n",
    "    img = Input(shape=input_shape)\n",
    "    y = conv(img, dis_n_filters)\n",
    "    y = conv(y, dis_n_filters*2)\n",
    "    y = conv(y, dis_n_filters*4)\n",
    "    y = conv(y, dis_n_filters*8)\n",
    "    if patchgan:\n",
    "        y = LeakyReLU(alpha=0.2)(y)\n",
    "        output = Conv2D(1, kernel_size=kernel_size, strides=1, padding='same')(y)\n",
    "    else:\n",
    "        y = Flatten()(y)\n",
    "        y = Dense(1)(y)\n",
    "        output = Activation('linear')(y)\n",
    "    discriminator = Model(img, output)\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "outside-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cyclegan(shapes, dis_n_filters, gen_n_filters, kernel_size, unet=True, patchgan=False, identity=False):\n",
    "    A_shape, B_shape = shapes\n",
    "    d_A = build_discriminator(A_shape, kernel_size, dis_n_filters, patchgan=patchgan)\n",
    "    d_B = build_discriminator(B_shape, kernel_size, dis_n_filters, aptchgan=patchgan)\n",
    "    d_A.summary()\n",
    "    d_B.summary()\n",
    "    d_A.compile(loss='mse', optimizer=RMSprop(lr=2e-4, decay=6e-8))\n",
    "    d_B.compile(loss='mse', optimizer=RMSprop(lr=2e-4, decay=6e-8))\n",
    "    if unet:\n",
    "        g_AB = build_generator_unet(A_shape, B_shape, kernel_size, gen_n_filters)\n",
    "        g_BA = build_generator_unet(B_shape, A_shape, kernel_size, gen_n_filters)\n",
    "    else:\n",
    "        g_AB = build_generator_resnet(A_shape, B_shape, kernel_size, gen_n_filters)\n",
    "        g_BA = build_generator_resnet(B_shape, A_shape, kernel_size, gen_n_filters)\n",
    "    d_A.trainable = False\n",
    "    d_B.trainable = False\n",
    "    g_AB.summary()\n",
    "    g_BA.summary()\n",
    "    img_A = Input(shape=A_shape)\n",
    "    img_B = Input(shape=B_shape)\n",
    "    fake_A = g_BA(img_B)\n",
    "    fake_B = g_AB(img_A)\n",
    "    valid_A = d_A(fake_A)\n",
    "    valid_B = d_B(fake_B)\n",
    "    recons_A = g_BA(fake_B)\n",
    "    recons_B = g_AB(fake_A)\n",
    "    \n",
    "    if identity:\n",
    "        img_A_id = g_BA(img_A)\n",
    "        img_B_id = g_AB(img_B)\n",
    "        inputs = [img_A, img_B]\n",
    "        outputs = [valid_A, valid_B, recons_A, recons_B, img_A_id, img_B_id]\n",
    "        loss = ['mse', 'mse', 'mae', 'mae', 'mae', 'mae']\n",
    "        loss_weights = [1., 1., 10., 10., 0.5, 0.5]\n",
    "    else:\n",
    "        inputs = [img_A, img_B]\n",
    "        outputs = [valid_A, valid_B, recons_A, recons_B]\n",
    "        loss = ['mse', 'mse', 'mae', 'mae']\n",
    "        loss_weights = [1., 1., 10., 10.]\n",
    "    adv = Model(inputs, outputs)\n",
    "    adv.compile(loss=loss, optimizer=RMSprop(lr=2e-4, decay=6e-8), loss_weights=loss_weights)\n",
    "    return d_A, d_B, g_AB, g_BA, adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-investment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
