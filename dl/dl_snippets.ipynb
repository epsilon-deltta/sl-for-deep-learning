{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-mobility",
   "metadata": {},
   "source": [
    "#### gpu\n",
    "[link](https://mkbahk.medium.com/keras-tensorflow%ED%99%98%EA%B2%BD%EC%97%90%EC%84%9C-%EB%8B%A4%EC%A4%91-cpu%EB%B6%84%EC%82%B0-%EB%8B%A4%EC%A4%91-nvidia-gpu%EB%B6%84%EC%82%B0%EC%B2%98%EB%A6%AC-%EB%8B%A4%EC%A4%91google-tup%EB%B6%84%EC%82%B0-%EB%8B%A4%EC%A4%91-graphcore-ipu%EB%B6%84%EC%82%B0-%EB%B3%91%EB%A0%AC-%EC%B2%98%EB%A6%AC-%EC%84%A4%EC%A0%95%EB%B2%95-79b32f69c9f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_logical_devices(“GPU”)\n",
    "if len(gpus) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n",
    "    print(‘\\n\\nRunning on multiple GPUs ‘, [gpu.name for gpu in gpus])\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
    "with strategy.scope():\n",
    "  # cnn w/o pre-emb\n",
    "    model0 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "chemical-essex",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fRU709pP_xJZ",
    "outputId": "b4c5c683-8916-4457-d882-3eee80f0db56"
   },
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-hello",
   "metadata": {},
   "source": [
    "### total (cnn ,callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-testimony",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D ,Conv1D\n",
    "from keras.layers import Embedding ,Flatten\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# seed 값 설정\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "# 데이터 불러오기\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_test = np_utils.to_categorical(Y_test)\n",
    "\n",
    "# 컨볼루션 신경망의 설정\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(28, 28, 1), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,  activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# 모델 최적화 설정\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# 모델의 실행\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=50, batch_size=200, verbose=1,\n",
    "#                     callbacks=[early_stopping_callback,checkpointer])\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-prairie",
   "metadata": {},
   "source": [
    "### preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-blink",
   "metadata": {},
   "source": [
    "#### missing value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Missing Values\n",
    "\n",
    "# Function to calculate missing values by column\n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns\n",
    "\n",
    "missing_values_table(data).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-winning",
   "metadata": {},
   "source": [
    "#### Collinearity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "### removing features over threshold\n",
    "def remove_collinear_features(x, threshold):\n",
    "    '''\n",
    "    Objective:\n",
    "        Remove collinear features in a dataframe with a correlation coefficient\n",
    "        greater than the threshold. Removing collinear features can help a model\n",
    "        to generalize and improves the interpretability of the model.\n",
    "        \n",
    "    Inputs: \n",
    "        threshold: any features with correlations greater than this value are removed\n",
    "    \n",
    "    Output: \n",
    "        dataframe that contains only the non-highly-collinear features\n",
    "    '''\n",
    "    \n",
    "    # Dont want to remove correlations between loss\n",
    "    y = x['loss']\n",
    "    x = x.drop(columns = ['loss'])\n",
    "    \n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = x.corr()\n",
    "    iters = range(len(corr_matrix.columns) - 1)\n",
    "    drop_cols = []\n",
    "\n",
    "    # Iterate through the correlation matrix and compare correlations\n",
    "    for i in iters:\n",
    "        for j in range(i):\n",
    "            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "            col = item.columns\n",
    "            row = item.index\n",
    "            val = abs(item.values)\n",
    "            \n",
    "            # If correlation exceeds the threshold\n",
    "            if val >= threshold:\n",
    "                # Print the correlated features and the correlation value\n",
    "                # print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
    "                drop_cols.append(col.values[0])\n",
    "\n",
    "    # Drop one of each pair of correlated columns\n",
    "    drops = set(drop_cols)\n",
    "    x = x.drop(columns = drops)\n",
    "    \n",
    "    # Add the score back in to the data\n",
    "    x['loss'] = y\n",
    "               \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-female",
   "metadata": {},
   "source": [
    "#### Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-specialist",
   "metadata": {},
   "source": [
    "#### One-Hot-Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num category\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "df = to_categorical(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string category\n",
    "labels_ohe_names = pd.get_dummies(target_labels, sparse=True)\n",
    "labels_ohe = np.asarray(labels_ohe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-hammer",
   "metadata": {},
   "source": [
    "#### visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    t = f.suptitle('Deep Neural Net Performance', fontsize=12)\n",
    "    f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "    epochs = list(range(len(hists[i]['acc']) )  )\n",
    "    ax1.plot(epochs, hists[i]['acc'], label='Train Accuracy')\n",
    "    ax1.plot(epochs, hists[i]['val_acc'], label='Validation Accuracy')\n",
    "    ax1.set_xticks(epochs)\n",
    "    ax1.set_ylabel('Accuracy Value')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_title('Accuracy')\n",
    "    l1 = ax1.legend(loc='best')\n",
    "\n",
    "    ax2.plot(epochs, hists[i]['loss'], label='Train loss')\n",
    "    ax2.plot(epochs, hists[i]['val_loss'], label='Validation loss')\n",
    "    ax2.set_xticks(epochs)\n",
    "    ax2.set_ylabel('loss Value')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_title('loss')\n",
    "    l2 = ax2.legend(loc='best')\n",
    "    f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-logistics",
   "metadata": {},
   "source": [
    "### Overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (data augmentation)\n",
    "# dropout\n",
    "# BatchNormalization\n",
    "# regularizer\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dense(100,kernel_regularizer=tf.keras.regularizers.l2(0.001) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-medium",
   "metadata": {},
   "source": [
    "### vision "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-composer",
   "metadata": {},
   "source": [
    "#### imgs to Array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "imgarr = [img_to_array(load_img(img, target_size=(299, 299) )  ) for img in imglist]\n",
    "imgarr = np.asarray(imgarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pil\n",
    "imgpath = './data/shoppingimg/'\n",
    "imglist = [imgpath+name for name in os.listdir(imgpath)]\n",
    "\n",
    "imgarr = [np.asarray(Image.open(path) ) for path in imglist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-bones",
   "metadata": {},
   "source": [
    "### model load & save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "\n",
    "model.save('my_model.h5')\n",
    "model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-clock",
   "metadata": {},
   "source": [
    "### data load "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-interim",
   "metadata": {},
   "source": [
    "#### keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  'boston_housing',\n",
    "#  'cifar10',\n",
    "#  'cifar100',\n",
    "#  'fashion_mnist',\n",
    "#  'imdb',\n",
    "#  'mnist',\n",
    "#  'reuters'\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dropout,Flatten,Dense\n",
    "import numpy as np\n",
    "\n",
    "# seed 값 설정\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "# 데이터 불러오기\n",
    "\n",
    "(xtr, ytr), (xte, yte) = tf.keras.datasets.mnist.load_data()\n",
    "xtr = xtr.reshape(xtr.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "xte = xte.reshape(xte.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "ytr = tf.keras.utils.to_categorical(ytr)\n",
    "yte = tf.keras.utils.to_categorical(yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-basis",
   "metadata": {},
   "source": [
    "#### scikit learn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-transport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris preprocessing\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.DataFrame(data=iris.data,columns=iris.feature_names)\n",
    "\n",
    "## Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "df = sc.fit_transform(df)\n",
    "df = pd.DataFrame(df,columns=iris.feature_names)\n",
    "\n",
    "df['target'] = iris.target\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "target = enc.fit_transform(iris.target.reshape(-1,1)).toarray()\n",
    "\n",
    "xtr,xte, ytr,yte = train_test_split(df.iloc[:,:-1], target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-insertion",
   "metadata": {},
   "source": [
    "#### seaborn \n",
    "[data](https://github.com/mwaskom/seaborn-data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "df = sns.load_dataset('mpg') # iris "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['mpg','cylinders','displacement','horsepower','weight',\n",
    "              'acceleration','model year','origin','name'] \n",
    "# horsepower 열의 자료형 변경 (문자열 ->숫자)\n",
    "df['horsepower'].replace('?', np.nan, inplace=True)      # '?'을 np.nan으로 변경\n",
    "df.dropna(subset=['horsepower'], axis=0, inplace=True)   # 누락데이터 행을 삭제\n",
    "df['horsepower'] = df['horsepower'].astype('float')      # 문자열을 실수형으로 변환\n",
    "# 분석에 활용할 열(속성)을 선택 (연비, 실린더, 출력, 중량)\n",
    "ndf = df[['mpg', 'cylinders', 'horsepower', 'weight']]\n",
    "\n",
    "X=ndf[['cylinders', 'horsepower', 'weight']]  #독립 변수 X1, X2, X3\n",
    "y=ndf['mpg']     #종속 변수 Y\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10) \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()   \n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "r_square = lr.score(X_test, y_test)\n",
    "print(r_square) #  결정계수(R-제곱) 계산\n",
    "print('X 변수의 계수 a: ', lr.coef_) # 회귀식의 기울기\n",
    "print('상수항 b', lr.intercept_) # 회귀식의 y절편\n",
    "\n",
    "# train data의 산점도와 test data로 예측한 회귀선을 그래프로 출력 \n",
    "y_hat = lr.predict(X_test)\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax1 = sns.distplot(y_test, hist=False, label=\"y_test\")\n",
    "ax2 = sns.distplot(y_hat, hist=False, label=\"y_hat\", ax=ax1)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-brush",
   "metadata": {},
   "source": [
    "### pca "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "comp_n = 2\n",
    "pca = PCA(n_components=comp_n) # 주성분을 몇개로 할지 결정\n",
    "cols = []\n",
    "for x in range(comp_n):\n",
    "    cols.append('comp_%d'%x)\n",
    "printcipalComponents = pca.fit_transform(X)\n",
    "principalDf = pd.DataFrame(data=printcipalComponents, columns = cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-tampa",
   "metadata": {},
   "source": [
    "### Machine Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-fitting",
   "metadata": {
    "_uuid": "59165d3c0c09f25fa49a8b70e13e057a75ebd662"
   },
   "outputs": [],
   "source": [
    "# # # Models to Evaluate\n",
    "\n",
    "# We will compare five different machine learning Cassification models:\n",
    "\n",
    "# 1 - Logistic Regression\n",
    "# 2 - K-Nearest Neighbors Classification\n",
    "# 3 - Suport Vector Machine\n",
    "# 4 - Naive Bayes\n",
    "# 5 - Random Forest Classification\n",
    "\n",
    "# Function to calculate mean absolute error\n",
    "def cross_val(X_train, y_train, model):\n",
    "    # Applying k-Fold Cross Validation\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    accuracies = cross_val_score(estimator = model, X = X_train, y = y_train, cv = 5)\n",
    "    return accuracies.mean()\n",
    "\n",
    "# Takes in a model, trains the model, and evaluates the model on the test set\n",
    "def fit_and_evaluate(model):\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions and evalute\n",
    "    model_pred = model.predict(X_test)\n",
    "    model_cross = cross_val(X_train, y_train, model)\n",
    "    \n",
    "    # Return the performance metric\n",
    "    return model_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-advertiser",
   "metadata": {
    "_uuid": "a5d839ce0829f4d6205b69767b7fa84b0d547744"
   },
   "outputs": [],
   "source": [
    "# # Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "naive = GaussianNB()\n",
    "naive_cross = fit_and_evaluate(naive)\n",
    "\n",
    "print('Naive Bayes Performance on the test set: Cross Validation Score = %0.4f' % naive_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as dtc\n",
    "dtcr =dtc(criterion='entropy', max_depth=5)\n",
    "dtc_cross = fit_and_evaluate(random)\n",
    "\n",
    "print('decision Tree Performance on the test set: Cross Validation Score = %0.4f' % dtc_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-center",
   "metadata": {
    "_uuid": "e90ced9e87d28f2532dd5f9856e41d59e52ed11b"
   },
   "outputs": [],
   "source": [
    "# # Random Forest Classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')\n",
    "random_cross = fit_and_evaluate(random)\n",
    "\n",
    "print('Random Forest Performance on the test set: Cross Validation Score = %0.4f' % random_cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-peninsula",
   "metadata": {
    "_uuid": "687f4e0ee16b1605f62da51937433c6b1fb1da19"
   },
   "outputs": [],
   "source": [
    "# # Gradiente Boosting Classification\n",
    "from xgboost import XGBClassifier\n",
    "gb = XGBClassifier()\n",
    "gb_cross = fit_and_evaluate(gb)\n",
    "\n",
    "print('Gradiente Boosting Classification Performance on the test set: Cross Validation Score = %0.4f' % gb_cross)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-discipline",
   "metadata": {},
   "source": [
    "#### ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cancer_data = load_breast_cancer()\n",
    "\n",
    "X_data = cancer_data.data\n",
    "y_label = cancer_data.target\n",
    "\n",
    "X_training , X_testing , y_training , y_testing = train_test_split(X_data , y_label , test_size=0.2 , random_state=0)\n",
    "\n",
    "# 개별 ML 모델을 위한 Classifier 생성.\n",
    "knn_clf  = KNeighborsClassifier(n_neighbors=4) #K최근접이웃\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=0)#랜덤포레스트\n",
    "dt_clf = DecisionTreeClassifier() #결정트리\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100) #아다부스트\n",
    "\n",
    "# 개별 모델들을 학습. \n",
    "knn_clf.fit(X_training, y_training)  \n",
    "rf_clf.fit(X_training , y_training)  \n",
    "dt_clf.fit(X_training , y_training)\n",
    "ada_clf.fit(X_training, y_training)\n",
    "\n",
    "# 학습된 개별 모델들이 각자 반환하는 예측 데이터 셋을 생성하고 개별 모델의 정확도 측정. \n",
    "knn_pred = knn_clf.predict(X_testing)\n",
    "rf_pred = rf_clf.predict(X_testing)\n",
    "dt_pred = dt_clf.predict(X_testing)\n",
    "ada_pred = ada_clf.predict(X_testing)\n",
    "\n",
    "print('KNN 정확도: {0:.4f}'.format(accuracy_score(y_testing, knn_pred)))\n",
    "print('랜덤 포레스트 정확도: {0:.4f}'.format(accuracy_score(y_testing, rf_pred)))\n",
    "print('결정 트리 정확도: {0:.4f}'.format(accuracy_score(y_testing, dt_pred)))\n",
    "print('에이다부스트 정확도: {0:.4f} :'.format(accuracy_score(y_testing, ada_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-taiwan",
   "metadata": {},
   "source": [
    "### model_selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# 10개의 파일로 쪼갬\n",
    "n_fold = 2\n",
    "skf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "for train, test in skf.split(X, Y):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=60, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X[train], Y[train], epochs=100, batch_size=5)\n",
    "    k_accuracy = \"%.4f\" % (model.evaluate(X[test], Y[test])[1])\n",
    "    accuracy.append(k_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k folding\n",
    "from sklearn.model_selection import KFold\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores=[]\n",
    "\n",
    "for train_idx, val_idx in k_fold.split(X):\n",
    "    # split train, validation set\n",
    "    xtr = X.iloc[train_idx]\n",
    "    ytr = y.iloc[train_idx]\n",
    "    xte = X.iloc[val_idx]\n",
    "    yte = y.iloc[val_idx]\n",
    "    lr = lrg()\n",
    "    lr.fit(xtr,ytr)\n",
    "    score = lr.score(xte,yte)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data 와 test data로 구분(7:3 비율) X,y : DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtr, xte, ytr, yte = train_test_split(x, y, test_size=0.3, random_state=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-triumph",
   "metadata": {},
   "source": [
    "### auto-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-memorabilia",
   "metadata": {},
   "source": [
    "#### Bayesian optimization (lightGBM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Bayesian-Optimization\n",
    "import lightgbm as lgb\n",
    "lgb.LGBMClassifier()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    " \n",
    "# 인위적 군집 데이터 생성\n",
    "X, y = make_classification(n_samples = 500, n_informative=2, n_features=5, flip_y = 0.1, random_state=0) \n",
    "rgb = np.array(['r', 'b'])\n",
    "plt.scatter(X[:, 0], X[:, 1], color=rgb[y]) # 생성된 X 데이터의 1,2열로 산점도를 그리고 색으로 그룹 구분\n",
    " \n",
    "# 데이터 프레임 형태로 변경.\n",
    "df = pd.DataFrame(X,columns=['x1','x2','x3','x4','x5'])\n",
    "df['y'] = y\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    " \n",
    "def lgb_evaluate(numLeaves, maxDepth, scaleWeight, minChildWeight, subsample, colSam, output = 'score'):\n",
    "    reg=lgb.LGBMRegressor(num_leaves=31, max_depth= 2,scale_pos_weight= scaleWeight, min_child_weight= minChildWeight, subsample= 0.4, colsample_bytree= 0.4, learning_rate=0.05,   n_estimators=20)\n",
    "    scores = cross_val_score(reg, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    # scores = cross_val_score(reg, train_x, train_y, cv=5, scoring='neg_mean_squared_error')\n",
    " \n",
    "    if output == 'score' :\n",
    "      return np.mean(scores)\n",
    "    if output == 'model' :\n",
    "      return reg\n",
    " \n",
    "def bayesOpt(train_x, train_y):\n",
    "    lgbBO = BayesianOptimization(lgb_evaluate, {  'numLeaves':  (5, 90),  'maxDepth': (2, 90),   'scaleWeight': (1, 10000),  'minChildWeight': (0.01, 70), 'subsample': (0.4, 1), 'colSam': (0.4, 1) })\n",
    "    lgbBO.maximize(init_points=5, n_iter=30)\n",
    "    print(lgbBO.res)\n",
    "    return lgbBO\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "X, y = np.array(df.drop('y',axis=1)), np.array(df['y'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "lgbBO = bayesOpt(X_train, y_train)\n",
    "\n",
    "# 최적 파라메터 저장 \n",
    "params = lgbBO.max['params']\n",
    " \n",
    "# 모델에 적용\n",
    "model = lgb_evaluate(\n",
    "    numLeaves = params['numLeaves'],\n",
    "     maxDepth = params['maxDepth'],\n",
    "     scaleWeight = params['maxDepth'],\n",
    "     minChildWeight = params['minChildWeight'],\n",
    "     subsample = params['subsample'],\n",
    "     colSam = params['colSam'],\n",
    "     output = 'model'\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-baseline",
   "metadata": {},
   "source": [
    "### Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "# epoch val_accuracy val_loss\n",
    "modelpath=\"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "### tensorboard\n",
    "import tensorflow as tf # !taskkill -pid  16224 /f\n",
    "%load_ext tensorboard \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "model.fit(x=x_train, \n",
    "          y=y_train, \n",
    "          epochs=5, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[tensorboard_callback])\n",
    "%tensorboard --logdir logs/fit --port 6006\n",
    "!tensorboard dev upload \\\n",
    "  --logdir logs/fit \\\n",
    "  --name \"(optional) My latest experiment\" \\\n",
    "  --description \"(optional) Simple comparison of several hyperparameters\" \\\n",
    "  --one_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-messaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "### list\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "callback_list = [\n",
    "  keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', # 모델의 검증 정확도 모니터링\n",
    "    patience=1, # 1 에포크보다 더 길게 향상되지 않으면 중단\n",
    "  ),\n",
    "  keras.callbacks.ModelCheckpoint(\n",
    "    filepath='my_model.h5', # 저장\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True, # 가장 좋은 모델\n",
    "  ),\n",
    "  keras.callbacks.ReduceLROnPlateau(\n",
    "  monitor='val_loss',\n",
    "  factor=0.1, # 콜백이 호출되면 학습률을 10배로 줄임\n",
    "  patience=10,\n",
    "  ),\n",
    "  tensorboard_callback\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30, batch_size=200, verbose=0, callbacks=[early_stopping_callback,checkpointer])\n",
    "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(X_test, Y_test)[1]))\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-trauma",
   "metadata": {},
   "source": [
    "### LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "inputs = tf.random.normal([32, 10, 8])\n",
    "# last hidden\n",
    "lstm = tf.keras.layers.LSTM(4)\n",
    "output = lstm(inputs)\n",
    "print(output.shape)\n",
    "#  sequence ,hidden ,carry\n",
    "lstm = tf.keras.layers.LSTM(4, return_sequences=True,return_state=True)\n",
    "whole_seq_output,hidden,carry= lstm(inputs)\n",
    "# sequence\n",
    "lstm = tf.keras.layers.LSTM(4, return_sequences=True)\n",
    "whole_seq_output = lstm(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-acrylic",
   "metadata": {},
   "source": [
    "#### many to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1.], [2.], [3.], [4.], [5.]]])\n",
    "y = np.array([[[2.], [3.], [4.], [5.], [6.]]])\n",
    "\n",
    "xInput = Input(batch_shape=(None, 5, 1))\n",
    "xLstm = LSTM(3, return_sequences=True)(xInput)\n",
    "xOutput = TimeDistributed(Dense(1))(xLstm)\n",
    "model = Model(xInput, xOutput)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "xInput = Input(batch_shape=(None, 5, 1))\n",
    "xLstm = LSTM(3, return_sequences=True)(xInput)\n",
    "xOutput = Dense(1)(xLstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "xInput = Input(batch_shape=(None, 5, 1))\n",
    "xLstm = LSTM(3, return_sequences=True)(xInput)\n",
    "xReduced = Lambda(lambda z: K.mean(z, axis=1))(xLstm)\n",
    "xOutput = Dense(1)(xReduced)\n",
    "model = Model(xInput, xOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-steal",
   "metadata": {},
   "source": [
    "#### one to many  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(self, ret_model = False):\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, input_shape = (224, 224, 3))\n",
    "        base_model.trainable=False\n",
    "        image_model = Sequential()\n",
    "        image_model.add(base_model)\n",
    "        image_model.add(Flatten())\n",
    "        image_model.add(Dense(EMBEDDING_DIM, input_dim = 4096, activation='relu'))\n",
    "\n",
    "        image_model.add(RepeatVector(self.max_cap_len))\n",
    "\n",
    "        lang_model = Sequential()\n",
    "        lang_model.add(Embedding(self.vocab_size, 256, input_length=self.max_cap_len))\n",
    "        lang_model.add(LSTM(256,return_sequences=True))\n",
    "        lang_model.add(TimeDistributed(Dense(EMBEDDING_DIM)))\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Merge([image_model, lang_model], mode='concat'))\n",
    "        model.add(LSTM(1000,return_sequences=False))\n",
    "        model.add(Dense(self.vocab_size))\n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        print \"Model created!\"\n",
    "\n",
    "        if(ret_model==True):\n",
    "            return model\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-karma",
   "metadata": {},
   "source": [
    "### Seq2Seq "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-electron",
   "metadata": {},
   "source": [
    "[실제 구현](https://wdprogrammer.tistory.com/37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# 입력 시퀀스의 정의와 처리\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# `encoder_outputs`는 버리고 상태(`state_h, state_c`)는 유지\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# `encoder_states`를 초기 상태로 사용해 decoder를 설정\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# 전체 출력 시퀀스를 반환하고 내부 상태도 반환하도록 decoder를 설정. \n",
    "# 학습 모델에서 상태를 반환하도록 하진 않지만, inference에서 사용할 예정.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "   \n",
    "# `encoder_input_data`와 `decoder_input_data`를 `decoder_target_data`로 반환하도록 모델을 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# 학습 실행\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # 상태 벡터로서 입력값을 encode\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # 길이가 1인 빈 목표 시퀀스를 생성\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # 대상 시퀀스 첫 번째 문자를 시작 문자로 기재.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # 시퀀스들의 batch에 대한 샘플링 반복(간소화를 위해, 배치 크기는 1로 상정)\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # 토큰으로 샘플링\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # 탈출 조건 : 최대 길이에 도달하거나\n",
    "        # 종료 문자를 찾을 경우\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # (길이 1인) 목표 시퀀스 최신화\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 상태 최신화\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-compensation",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컨볼루션 신경망의 설정\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(28, 28, 1), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,  activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-harrison",
   "metadata": {},
   "source": [
    "### GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, LeakyReLU, UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#이미지가 저장될 폴더가 없다면 만듭니다.\n",
    "import os\n",
    "if not os.path.exists(\"./gan_images\"):\n",
    "    os.makedirs(\"./gan_images\")\n",
    "\n",
    "np.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "#생성자 모델을 만듭니다.\n",
    "generator = Sequential()\n",
    "generator.add(Dense(128*7*7, input_dim=100, activation=LeakyReLU(0.2)))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Reshape((7, 7, 128)))\n",
    "generator.add(UpSampling2D())\n",
    "generator.add(Conv2D(64, kernel_size=5, padding='same'))\n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Activation(LeakyReLU(0.2)))\n",
    "generator.add(UpSampling2D())\n",
    "generator.add(Conv2D(1, kernel_size=5, padding='same', activation='tanh'))\n",
    "\n",
    "#판별자 모델을 만듭니다.\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Conv2D(64, kernel_size=5, strides=2, input_shape=(28,28,1), padding=\"same\"))\n",
    "discriminator.add(Activation(LeakyReLU(0.2)))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Conv2D(128, kernel_size=5, strides=2, padding=\"same\"))\n",
    "discriminator.add(Activation(LeakyReLU(0.2)))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Flatten())\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "discriminator.trainable = False\n",
    "\n",
    "#생성자와 판별자 모델을 연결시키는 gan 모델을 만듭니다.\n",
    "ginput = Input(shape=(100,))\n",
    "dis_output = discriminator(generator(ginput))\n",
    "gan = Model(ginput, dis_output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "gan.summary()\n",
    "\n",
    "#신경망을 실행시키는 함수를 만듭니다.\n",
    "def gan_train(epoch, batch_size, saving_interval):\n",
    "\n",
    "  # MNIST 데이터 불러오기\n",
    "\n",
    "  (X_train, _), (_, _) = mnist.load_data()  # 앞서 불러온 적 있는 MNIST를 다시 이용합니다. 단, 테스트과정은 필요없고 이미지만 사용할 것이기 때문에 X_train만 불러왔습니다.\n",
    "  X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "  X_train = (X_train - 127.5) / 127.5  # 픽셀값은 0에서 255사이의 값입니다. 이전에 255로 나누어 줄때는 이를 0~1사이의 값으로 바꾸었던 것인데, 여기서는 127.5를 빼준 뒤 127.5로 나누어 줌으로 인해 -1에서 1사이의 값으로 바뀌게 됩니다.\n",
    "  #X_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n",
    "\n",
    "  true = np.ones((batch_size, 1))\n",
    "  fake = np.zeros((batch_size, 1))\n",
    "\n",
    "  for i in range(epoch):\n",
    "          # 실제 데이터를 판별자에 입력하는 부분입니다.\n",
    "          idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "          imgs = X_train[idx]\n",
    "          d_loss_real = discriminator.train_on_batch(imgs, true)\n",
    "\n",
    "          #가상 이미지를 판별자에 입력하는 부분입니다.\n",
    "          noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "          gen_imgs = generator.predict(noise)\n",
    "          d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "\n",
    "          #판별자와 생성자의 오차를 계산합니다.\n",
    "          d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "          g_loss = gan.train_on_batch(noise, true)\n",
    "\n",
    "          print('epoch:%d' % i, ' d_loss:%.4f' % d_loss, ' g_loss:%.4f' % g_loss)\n",
    "\n",
    "        # 이부분은 중간 과정을 이미지로 저장해 주는 부분입니다. 본 장의 주요 내용과 관련이 없어\n",
    "        # 소스코드만 첨부합니다. 만들어진 이미지들은 gan_images 폴더에 저장됩니다.\n",
    "          if i % saving_interval == 0:\n",
    "              #r, c = 5, 5\n",
    "              noise = np.random.normal(0, 1, (25, 100))\n",
    "              gen_imgs = generator.predict(noise)\n",
    "\n",
    "              # Rescale images 0 - 1\n",
    "              gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "              fig, axs = plt.subplots(5, 5)\n",
    "              count = 0\n",
    "              for j in range(5):\n",
    "                  for k in range(5):\n",
    "                      axs[j, k].imshow(gen_imgs[count, :, :, 0], cmap='gray')\n",
    "                      axs[j, k].axis('off')\n",
    "                      count += 1\n",
    "              fig.savefig(\"gan_images/gan_mnist_%d.png\" % i)\n",
    "\n",
    "gan_train(4001, 32, 200)  #4000번 반복되고(+1을 해 주는 것에 주의), 배치 사이즈는 32,  200번 마다 결과가 저장되게 하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-canal",
   "metadata": {},
   "source": [
    "### AutoEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#MNIST데이터 셋을 불러옵니다.\n",
    "\n",
    "(X_train, _), (X_test, _) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "\n",
    "#생성자 모델을 만듭니다.\n",
    "autoencoder = Sequential()\n",
    "\n",
    "# 인코딩 부분입니다.\n",
    "autoencoder.add(Conv2D(16, kernel_size=3, padding='same', input_shape=(28,28,1), activation='relu'))\n",
    "autoencoder.add(MaxPooling2D(pool_size=2, padding='same'))\n",
    "autoencoder.add(Conv2D(8, kernel_size=3, activation='relu', padding='same'))\n",
    "autoencoder.add(MaxPooling2D(pool_size=2, padding='same'))\n",
    "autoencoder.add(Conv2D(8, kernel_size=3, strides=2, padding='same', activation='relu'))\n",
    "\n",
    "# 디코딩 부분이 이어집니다. \n",
    "autoencoder.add(Conv2D(8, kernel_size=3, padding='same', activation='relu'))\n",
    "autoencoder.add(UpSampling2D())\n",
    "autoencoder.add(Conv2D(8, kernel_size=3, padding='same', activation='relu'))\n",
    "autoencoder.add(UpSampling2D())\n",
    "autoencoder.add(Conv2D(16, kernel_size=3, activation='relu'))\n",
    "autoencoder.add(UpSampling2D())\n",
    "autoencoder.add(Conv2D(1, kernel_size=3, padding='same', activation='sigmoid'))\n",
    "\n",
    "# 전체 구조를 확인해 봅니다.\n",
    "autoencoder.summary()\n",
    "\n",
    "# 컴파일 및 학습을 하는 부분입니다.\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=128, validation_data=(X_test, X_test))\n",
    "\n",
    "#학습된 결과를 출력하는 부분입니다.\n",
    "random_test = np.random.randint(X_test.shape[0], size=5)  #테스트할 이미지를 랜덤하게 불러옵니다.\n",
    "ae_imgs = autoencoder.predict(X_test)  #앞서 만든 오토인코더 모델에 집어 넣습니다.\n",
    "\n",
    "plt.figure(figsize=(7, 2))  #출력될 이미지의 크기를 정합니다.\n",
    "\n",
    "for i, image_idx in enumerate(random_test):    #랜덤하게 뽑은 이미지를 차례로 나열합니다.\n",
    "   ax = plt.subplot(2, 7, i + 1) \n",
    "   plt.imshow(X_test[image_idx].reshape(28, 28))  #테스트할 이미지를 먼저 그대로 보여줍니다.\n",
    "   ax.axis('off')\n",
    "   ax = plt.subplot(2, 7, 7 + i +1)\n",
    "   plt.imshow(ae_imgs[image_idx].reshape(28, 28))  #오토인코딩 결과를 다음열에 출력합니다.\n",
    "   ax.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-arena",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers, initializers, regularizers, metrics\n",
    "\n",
    "np.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                  horizontal_flip=True,     #수평 대칭 이미지를 50% 확률로 만들어 추가합니다.\n",
    "                                  width_shift_range=0.1,  #전체 크기의 10% 범위에서 좌우로 이동합니다.\n",
    "                                  height_shift_range=0.1, #마찬가지로 위, 아래로 이동합니다.\n",
    "                                  #rotation_range=5,\n",
    "                                  #shear_range=0.7,\n",
    "                                  #zoom_range=[0.9, 2.2],\n",
    "                                  #vertical_flip=True,\n",
    "                                  fill_mode='nearest') \n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "       './train',   #학습셋이 있는 폴더의 위치입니다.\n",
    "       target_size=(150, 150),\n",
    "       batch_size=5,\n",
    "       class_mode='binary')\n",
    "\n",
    "#테스트 셋은 이미지 부풀리기 과정을 진행하지 않습니다.\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)  \n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "       './test',   #테스트셋이 있는 폴더의 위치입니다.\n",
    "       target_size=(150, 150),\n",
    "       batch_size=5,\n",
    "       class_mode='binary')\n",
    "\n",
    "\n",
    "# 앞서 배운 CNN 모델을 만들어 적용해 보겠습니다.\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(150,150,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#모델을 컴파일 합니다. \n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=0.0002), metrics=['accuracy'])\n",
    "\n",
    "#모델을 실행합니다\n",
    "history = model.fit_generator(\n",
    "       train_generator,\n",
    "       steps_per_epoch=100,\n",
    "       epochs=20,\n",
    "       validation_data=test_generator,\n",
    "       validation_steps=10)\n",
    "\n",
    "#결과를 그래프로 표현하는 부분입니다.\n",
    "acc= history.history['accuracy']\n",
    "val_acc= history.history['val_accuracy']\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "x_len = np.arange(len(y_loss))  \n",
    "plt.plot(x_len, acc, marker='.', c=\"red\", label='Trainset_acc')\n",
    "plt.plot(x_len, val_acc, marker='.', c=\"lightcoral\", label='Testset_acc')\n",
    "plt.plot(x_len, y_vloss, marker='.', c=\"cornflowerblue\", label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c=\"blue\", label='Trainset_loss')\n",
    "\n",
    "plt.legend(loc='upper right') \n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss/acc')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-edinburgh",
   "metadata": {},
   "source": [
    "### Transfer Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import Input, models, layers, optimizers, metrics\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "np.random.seed(3)\n",
    "tf.compat.v1.set_random_seed(3)\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                  horizontal_flip=True,\n",
    "                                  width_shift_range=0.1,\n",
    "                                  height_shift_range=0.1,\n",
    "                                  fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "       'train',\n",
    "       target_size=(150, 150),\n",
    "       batch_size=5,\n",
    "       class_mode='binary')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "       'test',\n",
    "       target_size=(150, 150),\n",
    "       batch_size=5,\n",
    "       class_mode='binary')\n",
    "\n",
    "transfer_model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "transfer_model.trainable = False\n",
    "transfer_model.summary()\n",
    "\n",
    "finetune_model = models.Sequential()\n",
    "finetune_model.add(transfer_model)\n",
    "finetune_model.add(Flatten())\n",
    "finetune_model.add(Dense(64, activation='relu'))\n",
    "finetune_model.add(Dense(2, activation='softmax'))\n",
    "finetune_model.summary()\n",
    "\n",
    "finetune_model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=0.0002), metrics=['accuracy'])\n",
    "\n",
    "history = finetune_model.fit_generator(\n",
    "       train_generator,\n",
    "       steps_per_epoch=100,\n",
    "       epochs=20,\n",
    "       validation_data=test_generator,\n",
    "       validation_steps=4)\n",
    "\n",
    "acc= history.history['accuracy']\n",
    "val_acc= history.history['val_accuracy']\n",
    "y_vloss = history.history['val_loss']\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "# 그래프로 표현\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, acc, marker='.', c=\"red\", label='Trainset_acc')\n",
    "plt.plot(x_len, val_acc, marker='.', c=\"lightcoral\", label='Testset_acc')\n",
    "plt.plot(x_len, y_vloss, marker='.', c=\"cornflowerblue\", label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c=\"blue\", label='Trainset_loss')\n",
    "\n",
    "# 그래프에 그리드를 주고 레이블을 표시\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss/acc')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-parliament",
   "metadata": {},
   "source": [
    "### Generating Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs file path ,each num of imgs  -> then,it generates virtual imgs(in  path/virtual)\n",
    "def generate_imgs(path,imgsPerfile,generator=None):\n",
    "    import os\n",
    "    from keras.preprocessing.image import load_img, img_to_array,array_to_img,ImageDataGenerator\n",
    "    filenames = os.listdir(path)\n",
    "    vdir = os.path.join(path,'virtual')\n",
    "    if not os.path.exists(vdir):\n",
    "        os.mkdir(vdir)\n",
    "\n",
    "    li = []\n",
    "    for x in filenames:\n",
    "        print(os.path.join(path,x ))\n",
    "        filepath = os.path.join(path,x ) \n",
    "        if os.path.isdir(filepath):\n",
    "            continue\n",
    "        img  = load_img(filepath )\n",
    "        iarr = img_to_array(img)\n",
    "        iarr = iarr.reshape((1,)+iarr.shape )\n",
    "        li.append(iarr)\n",
    "    li = np.concatenate(li)\n",
    "    if generator == None :\n",
    "        dgen = ImageDataGenerator(rescale=1./255,\n",
    "                                  horizontal_flip=True,   #수평 대칭 이미지를 50% 확률로 만들어 추가합니다.\n",
    "                                  width_shift_range=0.1,  #전체 크기의 10% 범위에서 좌우로 이동합니다.\n",
    "                                  height_shift_range=0.1, #마찬가지로 위, 아래로 이동합니다.\n",
    "                                  rotation_range=90,\n",
    "                                  #shear_range=0.7,\n",
    "                                  zoom_range=[0.9, 2.2],\n",
    "                                  vertical_flip=True,\n",
    "                                  fill_mode='nearest') \n",
    "    else :\n",
    "        dgen = generator\n",
    "    i = 0\n",
    "    for batch in dgen.flow(li, batch_size=2,\n",
    "                              save_to_dir = path+'/virtual' , save_prefix='fake', save_format='jpg'):\n",
    "        i += 1\n",
    "        if i > imgsPerfile-1 :\n",
    "            break  # 이미지 n장을 생성하고 마칩니다\n",
    "    del li"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
