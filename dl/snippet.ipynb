{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "from sklearn import linear_model\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.regression.quantile_regression import QuantReg\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data 와 test data로 구분(7:3 비율) X,y : DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr0 = LinearRegression()\n",
    "lr0.fit(x_train,y_train)\n",
    "# lr0.coef_ , lr0.intercept_\n",
    "sco = lr0.score(x_test,y_test)\n",
    "lr0.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time as tt\n",
    "iterate_n = 10\n",
    "first = tt()\n",
    "for x in range(iterate_n) :\n",
    "#     train_err[train_err['user_id'].isin(strange_user_id)]\n",
    "duration = tt() -first\n",
    "print(\"duration : \" ,duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import json\n",
    "my_api_key = \"api key\"\n",
    "my_cse_id = \"custom search engine id\"  \n",
    "def google_search(search_term, api_key, cse_id, **kwargs):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()\n",
    "    return res\n",
    "//구글로 검색하기\n",
    "search_result = google_search(\"Python Google Custom Search\", my_api_key, my_cse_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('search_result_all.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(search_result, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫인코딩(더미 변수)\n",
    "from sklearn import preprocessing    \n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()     # label encoder 생성\n",
    "onehot_encoder = preprocessing.OneHotEncoder()   # one hot encoder 생성\n",
    "\n",
    "onehot_location = label_encoder.fit_transform(df['지역'])\n",
    "onehot_code = label_encoder.fit_transform(df['코드'])\n",
    "onehot_type = label_encoder.fit_transform(df['유형'])\n",
    "onehot_day = label_encoder.fit_transform(df['주야'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "xtr , xte,ytr,yte = tts(x,y,random_state=10,test_size=0.3)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier as dtc\n",
    "dtcr =dtc(criterion='entropy', max_depth=5)\n",
    "dtcr.fit(xtr,ytr)\n",
    "\n",
    "from sklearn import metrics \n",
    "tree_matrix = metrics.confusion_matrix(y_test, y_hat)  \n",
    "print(tree_matrix)\n",
    "print('\\n')\n",
    "\n",
    "# 모형 성능 평가 - 평가지표 계산\n",
    "tree_report = metrics.classification_report(y_test, y_hat)            \n",
    "print(tree_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores=[]\n",
    "\n",
    "for train_idx, val_idx in k_fold.split(X):\n",
    "#     print(train_idx,\" : \",val_idx)\n",
    "\n",
    "    # split train, validation set\n",
    "    xtr = X.iloc[train_idx]\n",
    "    ytr = y.iloc[train_idx]\n",
    "    xte = X.iloc[val_idx]\n",
    "    yte = y.iloc[val_idx]\n",
    "    lr = lrg()\n",
    "    lr.fit(xtr,ytr)\n",
    "    score = lr.score(xte,yte)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "comp_n = 2\n",
    "pca = PCA(n_components=comp_n) # 주성분을 몇개로 할지 결정\n",
    "cols = []\n",
    "for x in range(comp_n):\n",
    "    cols.append('comp_%d'%x)\n",
    "printcipalComponents = pca.fit_transform(X)\n",
    "principalDf = pd.DataFrame(data=printcipalComponents, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "colors = {-1:'gray', 0:'coral', 1:'blue', 2:'green', 3:'red', 4:'purple', \n",
    "          5:'orange', 6:'brown', 7:'brick', 8:'yellow', 9:'magenta', 10:'cyan',11:'pink'}\n",
    "\n",
    "df = itaca\n",
    "cluster2_map = folium.Map(location=[37.55,126.98], tiles='Stamen Terrain', \n",
    "                        zoom_start=12)\n",
    "\n",
    "for name, lat, lng in zip(df.상호명, df.위도, df.경도):  \n",
    "    folium.CircleMarker([lat, lng],\n",
    "                        radius=5,                   # 원의 반지름\n",
    "                        color=colors[1],         # 원의 둘레 색상\n",
    "                        fill=True,\n",
    "                        fill_color=colors[1],    # 원을 채우는 색\n",
    "                        fill_opacity=0.7,           # 투명도    \n",
    "                        popup=name\n",
    "    ).add_to(cluster2_map)\n",
    "cluster2_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cluster\n",
    "colors = {-1:'gray', 0:'coral', 1:'blue', 2:'green', 3:'red', 4:'purple', \n",
    "          5:'orange', 6:'brown', 7:'brick', 8:'yellow', 9:'magenta', 10:'cyan',11:'pink'}\n",
    "\n",
    "columns_list =['경도','위도']\n",
    "X3 = itaca.loc[:, columns_list]\n",
    "x = preprocessing.StandardScaler().fit(X3).transform(X3)\n",
    "\n",
    "# dbm = cluster.DBSCAN()#eps=0.2, min_samples=5\n",
    "dbm = cluster.KMeans(init='k-means++', n_clusters=5, n_init=10)\n",
    "dbm.fit(x)  \n",
    "\n",
    "itaca['cluster'] = dbm.labels_   \n",
    "\n",
    "cluster3_map = folium.Map(location=[37.55,126.98], tiles='Stamen Terrain', \n",
    "                        zoom_start=12)\n",
    "\n",
    "for name, lat, lng, clus in zip(itaca.상호명, itaca.위도, itaca.경도, itaca.cluster):  \n",
    "    folium.CircleMarker([lat, lng],\n",
    "                        radius=5,                   # 원의 반지름\n",
    "                        color=colors[clus],         # 원의 둘레 색상\n",
    "                        fill=True,\n",
    "                        fill_color=colors[clus],    # 원을 채우는 색\n",
    "                        fill_opacity=0.7,           # 투명도    \n",
    "                        popup=name\n",
    "    ).add_to(cluster3_map)\n",
    "cluster3_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cluster\n",
    "colors = {-1:'gray', 0:'coral', 1:'blue', 2:'green', 3:'red', 4:'purple', \n",
    "          5:'orange', 6:'brown', 7:'brick', 8:'yellow', 9:'magenta', 10:'cyan',11:'pink'}\n",
    "\n",
    "columns_list =['경도','위도']\n",
    "X3 = itaca.loc[:, columns_list]\n",
    "x = preprocessing.StandardScaler().fit(X3).transform(X3)\n",
    "\n",
    "# dbm = cluster.DBSCAN()#eps=0.2, min_samples=5\n",
    "dbm = cluster.KMeans(init='k-means++', n_clusters=5, n_init=10)\n",
    "dbm.fit(x)  \n",
    "\n",
    "itaca['cluster'] = dbm.labels_   \n",
    "\n",
    "cluster3_map = folium.Map(location=[37.55,126.98], tiles='Stamen Terrain', \n",
    "                        zoom_start=12)\n",
    "\n",
    "for name, lat, lng, clus in zip(itaca.상호명, itaca.위도, itaca.경도, itaca.cluster):  \n",
    "    folium.CircleMarker([lat, lng],\n",
    "                        radius=5,                   # 원의 반지름\n",
    "                        color=colors[clus],         # 원의 둘레 색상\n",
    "                        fill=True,\n",
    "                        fill_color=colors[clus],    # 원을 채우는 색\n",
    "                        fill_opacity=0.7,           # 투명도    \n",
    "                        popup=name\n",
    "    ).add_to(cluster3_map)\n",
    "cluster3_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "# 10개의 파일로 쪼갬\n",
    "n_fold = 2\n",
    "skf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "\n",
    "\n",
    "# 모델의 설정, 컴파일, 실행\n",
    "for train, test in skf.split(X, Y):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=60, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X[train], Y[train], epochs=100, batch_size=5)\n",
    "    k_accuracy = \"%.4f\" % (model.evaluate(X[test], Y[test])[1])\n",
    "    accuracy.append(k_accuracy)\n",
    "\n",
    "model.save('sonar_model.h5')\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "exsonar = load_model('sonar_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "#from sklearn.ensemble import IsolationForest\n",
    "#from sklearn.svm import OneClassSVM\n",
    "\n",
    "outlier = LocalOutlierFactor()\n",
    "#outlier = IsolationForest()\n",
    "#outlier = OneClassSVM()\n",
    "\n",
    "y_predict = outlier.fit_predict(df_train.drop(['score'], axis=1))\n",
    "# print(y_predict) #[ 1  1  1  1  1  1  1 -1  1  1  1]\n",
    "df_train['outlier'] = y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='binary_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "# 모델 저장 폴더 설정\n",
    "MODEL_DIR = './model0/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "   os.mkdir(MODEL_DIR)\n",
    "\n",
    "# 모델 저장 조건 설정\n",
    "modelpath=MODEL_DIR+\"{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
    "# 모델 실행 및 저장\n",
    "history = model.fit(X, Y, validation_split=0.2, epochs=200, batch_size=200, verbose=0, callbacks=[checkpointer,early_stopping_callback])\n",
    "\n",
    "# y_vloss에 테스트셋으로 실험 결과의 오차 값을 저장\n",
    "y_vloss=history.history['val_loss']\n",
    "\n",
    "# y_acc 에 학습 셋으로 측정한 정확도의 값을 저장\n",
    "y_acc=history.history['val_accuracy']\n",
    "\n",
    "# x값을 지정하고 정확도를 파란색으로, 오차를 빨간색으로 표시\n",
    "x_len = numpy.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=3)\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# early stop\n",
    "# 결과 출력\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 코드 내부에 한글을 사용가능 하게 해주는 부분입니다.\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seed 값 설정\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "# 학습셋, 테스트셋 지정하기\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=5000)\n",
    "\n",
    "# 데이터 전처리\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=100)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=100)\n",
    "\n",
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(64, 5, padding='valid', activation='relu',strides=1))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(55))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# 모델의 컴파일\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델의 실행\n",
    "history = model.fit(x_train, y_train, batch_size=100, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "# 테스트 정확도 출력\n",
    "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(x_test, y_test)[1]))\n",
    "\n",
    "# 테스트 셋의 오차\n",
    "y_vloss = history.history['val_loss']\n",
    "\n",
    "# 학습셋의 오차\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "# 그래프로 표현\n",
    "x_len = numpy.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, marker='.', c=\"red\", label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c=\"blue\", label='Trainset_loss')\n",
    "\n",
    "# 그래프에 그리드를 주고 레이블을 표시\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "# 모델 저장 폴더 설정\n",
    "MODEL_DIR = './model0/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "   os.mkdir(MODEL_DIR)\n",
    "\n",
    "# 모델 저장 조건 설정\n",
    "modelpath=MODEL_DIR+\"{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
    "# 모델 실행 및 저장\n",
    "history = model.fit(X, Y, validation_split=0.2, epochs=200, batch_size=200, verbose=0, callbacks=[checkpointer,early_stopping_callback])\n",
    "\n",
    "# y_vloss에 테스트셋으로 실험 결과의 오차 값을 저장\n",
    "y_vloss=history.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =================================\n",
    "# ================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##numpy > onehotencode\n",
    "from keras.utils import np_utils\n",
    "Y_train = np_utils.to_categorical(Y_class_train, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
