{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 코드 내부에 한글을 사용가능 하게 해주는 부분입니다.\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Embedding\n",
    "\n",
    "#주어진 문장을 '단어'로 토큰화 하기\n",
    "# keras.preprocessing(.text ,.sequence ,.image)\n",
    "# keras.preprocessing.text \n",
    "# Tokenizer ,  hashing_trick , \n",
    "# one_hot , text_to_word_sequence ,tokenizer_from_json\n",
    "# 케라스의 텍스트 전처리와 관련한 함수중 text_to_word_sequence 함수를 불러 옵니다.\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence,Tokenizer\n",
    "\n",
    "from keras.preprocessing import text\n",
    "\n",
    "from keras.utils import to_categorical as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Life is too short to take this class....'\n",
    "result = text_to_word_sequence(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['먼저 텍스트의 각 단어를 나누어 토큰화 합니다.',\n",
    "       '텍스트의 단어로 토큰화 해야 딥러닝에서 인식됩니다.',\n",
    "       '토큰화 한 결과는 딥러닝에서 사용 할 수 있습니다.',\n",
    "       ]\n",
    "token = Tokenizer()             # 토큰화 함수 지정\n",
    "token.fit_on_texts(docs)       # 토큰화 함수에 문장 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n단어 카운트:\\n\", token.word_counts)\n",
    "print(\"\\n문장 카운트: \", token.document_count)\n",
    "print(\"\\n각 단어가 몇개의 문장에 포함되어 있는가:\\n\", token.word_docs)\n",
    "print(\"\\n각 단어에 매겨진 인덱스 값:\\n\",  token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"시발 웃다 눈 빠젔다 너무 재밌네요\",\"너무 최고예요\",\"참 잘 만든 영화예요\",\"추천하고 싶은 영화입니다\",\"한번 더 보고싶네요\",\"글쎄요\",\"별로예요\",\"생각보다 지루하네요\",\"연기가 어색해요\",\"재미없어요\"]\n",
    "classes = array([1,1,1,1,1,0,0,0,0,0])\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)\n",
    "print(token.word_index)\n",
    "\n",
    "x = token.texts_to_sequences(docs)\n",
    "# 패딩, 서로 다른 길이의 데이터를 4로 맞추어 줍니다.\n",
    "# [2, 3, 4, 5, 1, 6] -> [ 4  5  1  6]  숫자 넘으면 앞에꺼 자르고 뒤에서 n 개 가져옴\n",
    "# [1 7] -> [ 0  0  1  7]               앞 0 으로 채움\n",
    "padded_x = pad_sequences(x, 4)  \n",
    "print(\"\\n패딩 결과:\\n\", padded_x)\n",
    "#딥러닝 모델\n",
    "print(\"\\n딥러닝 모델 시작:\")\n",
    "#임베딩에 입력될 단어의 수를 지정합니다.\n",
    "word_size = len(token.word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 0s 992us/step - loss: 0.7035 - accuracy: 0.4000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7011 - accuracy: 0.4000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6988 - accuracy: 0.4000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6965 - accuracy: 0.4000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6942 - accuracy: 0.4000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6919 - accuracy: 0.4000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.6897 - accuracy: 0.4000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6874 - accuracy: 0.4000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6852 - accuracy: 0.6000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6829 - accuracy: 0.6000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6807 - accuracy: 0.7000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6785 - accuracy: 0.8000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6763 - accuracy: 0.9000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6741 - accuracy: 0.9000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6719 - accuracy: 0.9000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6697 - accuracy: 0.9000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6675 - accuracy: 0.9000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6653 - accuracy: 0.9000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6631 - accuracy: 0.9000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6609 - accuracy: 0.9000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6586 - accuracy: 0.9000\n",
      "\n",
      " Accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "#단어 임베딩을 포함하여 딥러닝 모델을 만들고 결과를 출력합니다.\n",
    "model = Sequential()\n",
    "model.add(Embedding(word_size, 8, input_length=4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(padded_x, classes, epochs=20)\n",
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(padded_x, classes)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### movie review classification (lstm , cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 코드 내부에 한글을 사용가능 하게 해주는 부분입니다.\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\epsilon\\dev\\anaconda\\envs\\kera\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "c:\\users\\epsilon\\dev\\anaconda\\envs\\kera\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "# seed 값 설정\n",
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "# 학습셋, 테스트셋 지정하기\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=5000)\n",
    "\n",
    "# 데이터 전처리\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=100)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(64, 5, padding='valid', activation='relu',strides=1))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(55))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# 모델의 컴파일\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델의 실행\n",
    "history = model.fit(x_train, y_train, batch_size=100, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "# 테스트 정확도 출력\n",
    "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(x_test, y_test)[1]))\n",
    "\n",
    "# 테스트 셋의 오차\n",
    "y_vloss = history.history['val_loss']\n",
    "\n",
    "# 학습셋의 오차\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "# 그래프로 표현\n",
    "x_len = numpy.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, marker='.', c=\"red\", label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c=\"blue\", label='Trainset_loss')\n",
    "\n",
    "# 그래프에 그리드를 주고 레이블을 표시\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words (DTM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록한다.\n",
    "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF (Term frequency - Inverse Document Frequency )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skelean: Corpus -> Tf-idf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "print(tfidfv.transform(corpus).toarray())\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28867513 0.         0.28867513 0.28867513 0.28867513 0.\n",
      "  0.         0.28867513 0.         0.28867513 0.28867513 0.57735027\n",
      "  0.28867513]\n",
      " [0.20039586 0.28164939 0.20039586 0.20039586 0.20039586 0.28164939\n",
      "  0.28164939 0.20039586 0.28164939 0.20039586 0.20039586 0.60118757\n",
      "  0.20039586]]\n",
      "{'you': 11, 'know': 2, 'want': 9, 'your': 12, 'love': 4, 'like': 3, 'what': 10, 'should': 7, 'do': 0, 'really': 6, 'very': 8, 'mych': 5, 'for': 1}\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "corpus0 = [\n",
    "    'you really know I want your love',\n",
    "    'I like you very mych',\n",
    "    'what should I do for you!!',    \n",
    "]\n",
    "docss =['. '.join(corpus),'. '.join(corpus0)]\n",
    "\n",
    "tfidfv = TfidfVectorizer().fit(docss)\n",
    "print(tfidfv.transform(docss).toarray())\n",
    "print(tfidfv.vocabulary_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
